{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a extrer la información de los diferentes datasets de amazon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def convert(x):\n",
    "    ''' Convert a json string to a flat python dictionary\n",
    "    which can be passed into Pandas. '''\n",
    "    ob = json.loads(x)\n",
    "    for k, v in ob.items():\n",
    "        if isinstance(v, list):\n",
    "            ob[k] = ','.join(str(v))\n",
    "        elif isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                ob['%s_%s' % (k, kk)] = vv\n",
    "            del ob[k]\n",
    "    return ob\n",
    "\n",
    "def convertAllJSON2CSV(): \n",
    "    for json_filename in glob('*.json'):\n",
    "        csv_filename = '%s.csv' % json_filename[:-5]\n",
    "        print 'Converting %s to %s' % (json_filename, csv_filename)\n",
    "        df = pd.DataFrame([convert(line) for line in file(json_filename)])\n",
    "        df.to_csv(csv_filename, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2b46f6d809e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moverallDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"overall\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"reviewText\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"overall\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"overall\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "overallDF = df[[\"overall\",\"reviewText\"]]\n",
    "\n",
    "data = df.groupby(\"overall\")[\"overall\"].count()\n",
    "plt.bar(data.index.values,data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cachedStopWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d0bf04c40b9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcachedStopWords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cachedStopWords' is not defined"
     ]
    }
   ],
   "source": [
    "cachedStopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hay que igualar y coger muestras de tamaño similar para todas las valoraciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.-  Procesamiento de texto sobre las reviews\n",
    "Eliminamos stopwords (TF-idf)\n",
    "aplicamos métodos de lematizacion - TextBlob, NLTK\n",
    "extraemos entidades (tokens)\n",
    "\n",
    "## 3.- Extracción de muestras de test y de entrenamiento para el modelo\n",
    "## 4.- Entrenamiento del modelo a partir de los datos de test.\n",
    "## 5.- Ejecución del modelo. Evaluación de resultados, comparación con otros modelos Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Word\n",
    "import nltk\n",
    "# nltk.download() hay que hacerlo la primera vez para cargar todos los corpus necesarios\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcion que elimina las stopwords y aplica lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "cachedStopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "babyDf = pd.DataFrame([convert(line) for line in file('reviews_Baby_5.json')])\n",
    "babyDf.to_csv(csv_filename, encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 498,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(text):  \n",
    "    #salida = text.translate(None, string.punctuation).lower()\n",
    "    salida = text.translate(None, string.punctuation)\n",
    "    return salida\n",
    "\n",
    "def lemmatizeReviewText(row):\n",
    "    words = TextBlob(row[\"review_clean\"]).words.lemmatize()\n",
    "    ext = ' '.join(word for word in words if word not in (cachedStopWords))\n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "babyDf = pd.DataFrame([convert(line) for line in file('reviews_Baby_5.json')])\n",
    "# forzamos la columna reviewText como str para el correcto funcionamiento de remove_punctuation\n",
    "babyDf['reviewText'] = babyDf['reviewText'].astype(str)\n",
    "babyDf['review_clean'] = babyDf['reviewText'].apply(remove_punctuation)\n",
    "babyDf['review_clean']= babyDf.apply(lemmatizeReviewText, axis=1)\n",
    "babyDf['review_clean']=babyDf['review_clean'].apply(lambda x: x.lower())\n",
    "\n",
    "babyDf.to_csv('reviews_Baby_5.csv', header=True,quoting=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tratamos el dataset de Pets\n",
    "petsDf = pd.DataFrame([convert(line) for line in file('reviews_Pet_Supplies_5.json')])\n",
    "# forzamos la columna reviewText como str para el correcto funcionamiento de remove_punctuation\n",
    "petsDf['reviewText'] = petsDf['reviewText'].astype(str)\n",
    "petsDf['review_clean'] = petsDf['reviewText'].apply(remove_punctuation)\n",
    "petsDf['review_clean']= petsDf.apply(lemmatizeReviewText, axis=1)\n",
    "petsDf['review_clean']= petsDf['review_clean'].apply(lambda x: x.lower())\n",
    "petsDf.to_csv('reviews_Pet_Supplies_5.csv', header=True,quoting=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generamos el dataframe principalDf a partir de los dataframes  petsDf y babyDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 601,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318628"
      ]
     },
     "execution_count": 601,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "\n",
    "principalDf = [babyDf,petsDf]\n",
    "babiesPetsDf = pd.concat(principalDf)\n",
    "# renombramos la columna overall por review_overall\n",
    "babiesPetsDf['review_overall']=babiesPetsDf['overall']\n",
    "del(babiesDf['overall'])\n",
    "len(babiesPetsDf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertimos el dataFrame al csv que sera la fuente de entrada al modelo de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "babiesPetsDf.to_csv('babies+pets_reviews.csv', header=True,quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268599    nothing great thing say blue buffalo product r...\n",
       "110798    attend work conference thought would purchase ...\n",
       "218728    love company quality product excellent custome...\n",
       "220497    competitor product wa dark color crumbly made ...\n",
       "314210    easy install look great color matching highlan...\n",
       "66635     nice soft tie easily gentle teething son gum p...\n",
       "73390     product worth super sturdy imagine going last ...\n",
       "186168    isnt favorite good variety diet girl like pate...\n",
       "191465    ok dog like favorite work toy use treat since ...\n",
       "284166    beautiful vivid color sturdy even withheld 4 m...\n",
       "Name: review_clean, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comprobamos que está bien generado el csv volviendolo a cargar en el DF\n",
    "import json\n",
    "import pandas as pd\n",
    "babiesPetsDf=pd.read_csv('babies+pets_reviews.csv',header=False,sep=',')\n",
    "babiesPetsDf['review_clean'].sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  crear un diccionario {palabra:ocurrencias}\n",
    "import numpy as np\n",
    "cnt = {}\n",
    "for linea in babiesPetsDf['review_clean'].astype(str).values:\n",
    "    for word in linea.split():\n",
    "        if (word not in cnt):\n",
    "            cnt[word] = 1\n",
    "        else:\n",
    "            cnt[word] += 1\n",
    "            \n",
    "wordsSerie=pd.Series(cnt,index=cnt.keys())\n",
    "wordsSerie=wordsSerie[wordsSerie.values>1]\n",
    "data={'a':wordsSerie.index,'b':wordsSerie.values}\n",
    "wordsDf=pd.DataFrame(data=data, index=np.arange(len(wordsSerie)))\n",
    "wordsDf=wordsDf.sort('b', ascending=False).head(1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generamos el array de palabras importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "important_words = str([str(s) for s in wordsDf['a']])\n",
    "f= open('important_words.json', 'w') \n",
    "f.write(important_words.replace(\"'\",'\"'))\n",
    "f.close()\n",
    "wordsDf.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Procesamiento de us_tweets.json --> us_tweets.csv\n",
    "conversión de us_tweets_json a us_tweets.csv para aplicar proceso de lematización y eliminación de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 741,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting USA_geo.json to USA_geo.csv\n",
      "9354\n"
     ]
    }
   ],
   "source": [
    "# utilizado para descartar caracteres no ASCII\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "# Devuelve un dataframe a partir del csv de reviews de Tweeter\n",
    "def getTweetsDF(json_filename):\n",
    "    csv_filename = '%s.csv' % json_filename[:-5]\n",
    "    print 'Converting %s to %s' % (json_filename, csv_filename)\n",
    "    df = pd.DataFrame( columns=['id','text','created_at','geo','city'])\n",
    "\n",
    "    json_data=open(json_filename).read()\n",
    "    ob = json.loads(json_data)\n",
    "    results = ob['results']\n",
    "    print(len(results))\n",
    "    for k  in results:\n",
    "\n",
    "        if (not k['geo']):\n",
    "            geo = ''\n",
    "        else:\n",
    "            idd = k['id_str']\n",
    "            #preguntamos si el texto está en ASCII para evitar que entren  caracteres UNICODE problematicos en NLP\n",
    "            if (is_ascii( k['text'])):\n",
    "                text = k['text']\n",
    "            else:\n",
    "                text = ''\n",
    "            created_at = k['created_at']\n",
    "            geo= ', '.join(map(str,k['geo']['coordinates']))\n",
    "            if (is_ascii(k['place']['full_name'])):\n",
    "                city = k['place']['full_name']\n",
    "            else:\n",
    "                city = ''\n",
    "            SR_row = pd.Series({'id':idd, 'text':text, 'created_at':created_at,'geo':'['+geo+']','city':city},name=len(df))\n",
    "            df=df.append(SR_row)\n",
    "    return df\n",
    "\n",
    "dfTweeter = getTweetsDF('USA_geo.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamos el dataset de us_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 742,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# lematiza y elimina Stop Words de la fila del dataset de reviews de Tweeter\n",
    "def lemmatizeReviewTextForTweets(row):\n",
    "\n",
    "\n",
    "    if (type(row['text']==str)):\n",
    "        words = TextBlob(row['text']).words.lemmatize()\n",
    "        ext = ' '.join(word for word in words if word not in (cachedStopWords))\n",
    "    else:\n",
    "        ext = ''\n",
    "    return ext\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 743,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n",
    "dfTweeter['text_clean'] = dfTweeter.apply(lemmatizeReviewTextForTweets,axis=1)\n",
    "dfTweeter['text_clean'] = dfTweeter['text_clean'].astype(str)\n",
    "dfTweeter['text_clean'] = dfTweeter['text_clean'].apply(remove_punctuation)\n",
    "# nos quedamos solo con los que tengan informado el campo text_clean\n",
    "dfTweeter=dfTweeter[dfTweeter['text_clean']!='']\n",
    "\n",
    "dfTweeter.to_csv('us_tweets.csv', header=True,quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 738,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'to_csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-738-5267a8bafbb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mdfTweeter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'us_tweets.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'to_csv'"
     ]
    }
   ],
   "source": [
    "dfTweeter.to_csv('us_tweets.csv', header=True,quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
