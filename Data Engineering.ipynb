{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a extrer la información de los diferentes datasets de amazon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def convert(x):\n",
    "    ''' Convert a json string to a flat python dictionary\n",
    "    which can be passed into Pandas. '''\n",
    "    ob = json.loads(x)\n",
    "    for k, v in ob.items():\n",
    "        if isinstance(v, list):\n",
    "            ob[k] = ','.join(str(v))\n",
    "        elif isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                ob['%s_%s' % (k, kk)] = vv\n",
    "            del ob[k]\n",
    "    return ob\n",
    "\n",
    "def convertAllJSON2CSV(): \n",
    "    for json_filename in glob('*.json'):\n",
    "        csv_filename = '%s.csv' % json_filename[:-5]\n",
    "        print 'Converting %s to %s' % (json_filename, csv_filename)\n",
    "        df = pd.DataFrame([convert(line) for line in file(json_filename)])\n",
    "        df.to_csv(csv_filename, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-2b46f6d809e7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'matplotlib inline'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0moverallDF\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"overall\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m\"reviewText\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"overall\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"overall\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "overallDF = df[[\"overall\",\"reviewText\"]]\n",
    "\n",
    "data = df.groupby(\"overall\")[\"overall\"].count()\n",
    "plt.bar(data.index.values,data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cachedStopWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d0bf04c40b9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcachedStopWords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cachedStopWords' is not defined"
     ]
    }
   ],
   "source": [
    "cachedStopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hay que igualar y coger muestras de tamaño similar para todas las valoraciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.-  Procesamiento de texto sobre las reviews\n",
    "Eliminamos stopwords (TF-idf)\n",
    "aplicamos métodos de lematizacion - TextBlob, NLTK\n",
    "extraemos entidades (tokens)\n",
    "\n",
    "## 3.- Extracción de muestras de test y de entrenamiento para el modelo\n",
    "## 4.- Entrenamiento del modelo a partir de los datos de test.\n",
    "## 5.- Ejecución del modelo. Evaluación de resultados, comparación con otros modelos Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download() hay que hacerlo la primera vez para cargar todos los corpus necesarios\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList([u'I', u'highly', u'recommend', u'this', u'series', u'It', u'is', u'a', u'must', u'for', u'anyone', u'who', u'is', u'yearning', u'to', u'watch', u'grown', u'up', u'television', u'Complex', u'character', u'and', u'plot', u'to', u'keep', u'one', u'totally', u'involved', u'Thank', u'you', u'Amazin', u'Prime'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(overallDF[\"reviewText\"][1]).words.lemmatize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcion que elimina las stopwords y aplica lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cachedStopWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-7-d0bf04c40b9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcachedStopWords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cachedStopWords' is not defined"
     ]
    }
   ],
   "source": [
    "cachedStopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overallDFsample=overallDF[1:5]\n",
    "overallDFsample['reviewTextLemmatized']=overallDFsample.apply(lemmatizeReviewText,axis=1)\n",
    "type (overallDFsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u'should',\n",
       " u'now',\n",
       " u'd',\n",
       " u'll',\n",
       " u'm',\n",
       " u'o',\n",
       " u're',\n",
       " u've',\n",
       " u'y',\n",
       " u'ain',\n",
       " u'aren',\n",
       " u'couldn',\n",
       " u'didn',\n",
       " u'doesn',\n",
       " u'hadn',\n",
       " u'hasn',\n",
       " u'haven',\n",
       " u'isn',\n",
       " u'ma',\n",
       " u'mightn',\n",
       " u'mustn',\n",
       " u'needn',\n",
       " u'shan',\n",
       " u'shouldn',\n",
       " u'wasn',\n",
       " u'weren',\n",
       " u'won',\n",
       " u'wouldn']"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachedStopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "babyDf = pd.DataFrame([convert(line) for line in file('reviews_Baby_5.json')])\n",
    "babyDf.to_csv(csv_filename, encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import string\n",
    "def remove_punctuation(text):  \n",
    "    salida = text.translate(None, string.punctuation).lower()\n",
    "    salida=salida.replace('ñ','n')\n",
    "    return salida\n",
    "\n",
    "def lemmatizeReviewText(row):\n",
    "    words = TextBlob(row[\"review_clean\"]).words.lemmatize()\n",
    "    ext = ' '.join(word for word in words if word not in (cachedStopWords))\n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "babyDf = pd.DataFrame([convert(line) for line in file('reviews_Baby_5.json')])\n",
    "# forzamos la columna reviewText como str para el correcto funcionamiento de remove_punctuation\n",
    "babyDf['reviewText'] = babyDf['reviewText'].astype(str)\n",
    "babyDf['review_clean'] = babyDf['reviewText'].apply(remove_punctuation)\n",
    "babyDf['review_clean']= babyDf.apply(lemmatizeReviewText, axis=1)\n",
    "babyDf['review_clean']=babyDf['review_clean'].apply(lambda x: x.lower())\n",
    "\n",
    "babyDf.to_csv('reviews_Baby_5.csv', header=True,quoting=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tratamos el dataset de Pets\n",
    "petsDf = pd.DataFrame([convert(line) for line in file('reviews_Pet_Supplies_5.json')])\n",
    "# forzamos la columna reviewText como str para el correcto funcionamiento de remove_punctuation\n",
    "petsDf['reviewText'] = petsDf['reviewText'].astype(str)\n",
    "petsDf['review_clean'] = petsDf['reviewText'].apply(remove_punctuation)\n",
    "petsDf['review_clean']= petsDf.apply(lemmatizeReviewText, axis=1)\n",
    "petsDf['review_clean']= petsDf['review_clean'].apply(lambda x: x.lower())\n",
    "petsDf.to_csv('reviews_Pet_Supplies_5.csv', header=True,quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318628"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenamos el dataset a partir de Pets y Babies\n",
    "\n",
    "frames = [babyDf,petsDf]\n",
    "babiesPetsDf = pd.concat(frames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convertimos el dataFrame al csv que sera la fuente de entrada al modelo de ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'babiesPetsDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1b541d61fb67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbabiesPetsDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'babies+pets_reviews.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'babiesPetsDf' is not defined"
     ]
    }
   ],
   "source": [
    "babiesPetsDf.to_csv('babies+pets_reviews.csv', header=True,quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268599    nothing great thing say blue buffalo product r...\n",
       "110798    attend work conference thought would purchase ...\n",
       "218728    love company quality product excellent custome...\n",
       "220497    competitor product wa dark color crumbly made ...\n",
       "314210    easy install look great color matching highlan...\n",
       "66635     nice soft tie easily gentle teething son gum p...\n",
       "73390     product worth super sturdy imagine going last ...\n",
       "186168    isnt favorite good variety diet girl like pate...\n",
       "191465    ok dog like favorite work toy use treat since ...\n",
       "284166    beautiful vivid color sturdy even withheld 4 m...\n",
       "Name: review_clean, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comprobamos que está bien generado el csv volviendolo a cargar en el DF\n",
    "import json\n",
    "import pandas as pd\n",
    "babiesPetsDf=pd.read_csv('babies+pets_reviews.csv',header=False,sep=',')\n",
    "babiesPetsDf['review_clean'].sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  crear un diccionario {palabra:ocurrencias}\n",
    "import numpy as np\n",
    "cnt = {}\n",
    "for linea in babiesPetsDf['review_clean'].astype(str).values:\n",
    "    for word in linea.split():\n",
    "        if (word not in cnt):\n",
    "            cnt[word] = 1\n",
    "        else:\n",
    "            cnt[word] += 1\n",
    "            \n",
    "wordsSerie=pd.Series(cnt,index=cnt.keys())\n",
    "wordsSerie=wordsSerie[wordsSerie.values>1]\n",
    "data={'a':wordsSerie.index,'b':wordsSerie.values}\n",
    "wordsDf=pd.DataFrame(data=data, index=np.arange(len(wordsSerie)))\n",
    "wordsDf=wordsDf.sort('b', ascending=False).head(1500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generamos el array de palabras importantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "important_words = str([str(s) for s in wordsDf['a']])\n",
    "f= open('important_words.json', 'w') \n",
    "f.write(important_words.replace(\"'\",'\"'))\n",
    "f.close()\n",
    "wordsDf.sample()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Procesamiento de us_tweets.json --> us_tweets.csv\n",
    "conversión de us_tweets_json a us_tweets.csv para aplicar proceso de lematización y eliminación de stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting USA_geo.json to USA_geo.csv\n",
      "9354\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def is_ascii(s):\n",
    "    return all(ord(c) < 128 for c in s)\n",
    "\n",
    "\n",
    "def getTweetsDF(json_filename):\n",
    "    csv_filename = '%s.csv' % json_filename[:-5]\n",
    "    print 'Converting %s to %s' % (json_filename, csv_filename)\n",
    "    df = pd.DataFrame( columns=['id','reviewText','created_at','geo','city'])\n",
    "\n",
    "    json_data=open(json_filename).read()\n",
    "    ob = json.loads(json_data)\n",
    "    results = ob['results']\n",
    "    print(len(results))\n",
    "    for k  in results:\n",
    "\n",
    "        if (not k['geo']):\n",
    "            geo = ''\n",
    "        else:\n",
    "            idd = k['id_str']\n",
    "            text = k['text']\n",
    "            created_at = k['created_at']\n",
    "            geo= ', '.join(map(str,k['geo']['coordinates']))\n",
    "            if (is_ascii(k['place']['full_name'])):\n",
    "                city = k['place']['full_name']\n",
    "            else:\n",
    "                city = ''\n",
    "            SR_row = pd.Series({'id':idd, 'reviewText':text, 'created_at':created_at,'geo':'['+geo+']','city':city},name=len(df))\n",
    "            df=df.append(SR_row)\n",
    "    return df\n",
    "\n",
    "dfTweeter = getTweetsDF('USA_geo.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tratamos el dataset de us_tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "(\"'str' object has no attribute 'words'\", u'occurred at index 0')",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-a854e02bcc25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mdfTweeter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reviewText'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfTweeter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reviewText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'ignore'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mdfTweeter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review_clean'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdfTweeter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reviewText'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mremove_punctuation\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 5\u001b[1;33m \u001b[0mdfTweeter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review_clean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdfTweeter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlemmatizeReviewText\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      6\u001b[0m \u001b[0mdfTweeter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review_clean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mdfTweeter\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review_clean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[0mdfTweeter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'us_tweets.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36mapply\u001b[1;34m(self, func, axis, broadcast, raw, reduce, args, **kwds)\u001b[0m\n\u001b[0;32m   3716\u001b[0m                     \u001b[1;32mif\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3717\u001b[0m                         \u001b[0mreduce\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3718\u001b[1;33m                     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_standard\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3719\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3720\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_apply_broadcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_apply_standard\u001b[1;34m(self, func, axis, ignore_failures, reduce)\u001b[0m\n\u001b[0;32m   3806\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3807\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3808\u001b[1;33m                     \u001b[0mresults\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3809\u001b[0m                     \u001b[0mkeys\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3810\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-70-2da885c46b83>\u001b[0m in \u001b[0;36mlemmatizeReviewText\u001b[1;34m(row)\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mlemmatizeReviewText\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;32mif\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"review_clean\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m''\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: (\"'str' object has no attribute 'words'\", u'occurred at index 0')"
     ]
    }
   ],
   "source": [
    "dfTweeter['reviewText'] = dfTweeter['reviewText'].apply(lambda x: x.encode('utf-8','ignore'))\n",
    "dfTweeter['review_clean'] = dfTweeter['reviewText'].apply(remove_punctuation)\n",
    "dfTweeter['review_clean']= dfTweeter.apply(lemmatizeReviewText, axis=1)\n",
    "dfTweeter['review_clean']= dfTweeter['review_clean'].apply(lambda x: x.lower())\n",
    "dfTweeter.to_csv('us_tweets.csv', header=True,quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def lemmatizeReviewTextForTweets(row):\n",
    "    a=row[\"review_clean\"].decode('unicode_escape').encode('ascii','ignore')\n",
    "    #a=row[\"review_clean\"].decode('unicode_escape').encode('utf-8','ignore')\n",
    "    words = TextBlob(a).words.lemmatize()\n",
    "    ext = ' '.join(word for word in words if word not in (cachedStopWords))\n",
    "    return ext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 385,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>reviewText</th>\n",
       "      <th>created_at</th>\n",
       "      <th>geo</th>\n",
       "      <th>city</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2842</th>\n",
       "      <td>737942098754818049</td>\n",
       "      <td>Cleared: Construction on #I495 WB from Exit 22...</td>\n",
       "      <td>Wed Jun 01 09:41:21 +0000 2016</td>\n",
       "      <td>[40.737598, -73.850601]</td>\n",
       "      <td>Queens, NY</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2843</th>\n",
       "      <td>737942099299950592</td>\n",
       "      <td>Supplemental Health Care #Healthcare #Job: Tra...</td>\n",
       "      <td>Wed Jun 01 09:41:21 +0000 2016</td>\n",
       "      <td>[38.4494063, -105.2253316]</td>\n",
       "      <td>Cañon City, CO</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      id                                         reviewText  \\\n",
       "2842  737942098754818049  Cleared: Construction on #I495 WB from Exit 22...   \n",
       "2843  737942099299950592  Supplemental Health Care #Healthcare #Job: Tra...   \n",
       "\n",
       "                          created_at                         geo  \\\n",
       "2842  Wed Jun 01 09:41:21 +0000 2016     [40.737598, -73.850601]   \n",
       "2843  Wed Jun 01 09:41:21 +0000 2016  [38.4494063, -105.2253316]   \n",
       "\n",
       "                city  \n",
       "2842      Queens, NY  \n",
       "2843  Cañon City, CO  "
      ]
     },
     "execution_count": 385,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTweeter[2842:2844]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dfTweeterSample=dfTweeter[0:2843]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Generamos us_tweets.csv procedente de Tweeter que se utilizará como entrada al modelo ML\n",
    "dfTweeter['reviewText'] = dfTweeter['reviewText'].apply(lambda x: x.encode('utf-8','ignore'))\n",
    "#dfTweeter['reviewText'] = dfTweeter['reviewText'].apply(lambda x: x.encode('ascii','ignore'))\n",
    "dfTweeter['review_clean'] = dfTweeter['reviewText'].apply(remove_punctuation)\n",
    "dfTweeter['review_clean']=dfTweeter.apply(lemmatizeReviewTextForTweets,axis=1)\n",
    "dfTweeter['review_clean']= dfTweeter['review_clean'].apply(lambda x: x.lower())\n",
    "dfTweeter.to_csv('us_tweets.csv', header=True,quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "del(dfTweeter['reviewT'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 416,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "is_ascii('cadddasnn')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
