{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vamos a extrer la informaci√≥n de los diferentes datasets de amazon "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting important_words.json to important_words.csv\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'items'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-8a4a279c9727>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[0mcsv_filename\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'%s.csv'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mjson_filename\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m     \u001b[1;32mprint\u001b[0m \u001b[1;34m'Converting %s to %s'\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mjson_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcsv_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m     \u001b[0mdf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mconvert\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mline\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mline\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mfile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjson_filename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m     \u001b[0mdf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsv_filename\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-1-8a4a279c9727>\u001b[0m in \u001b[0;36mconvert\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m      8\u001b[0m     which can be passed into Pandas. '''\n\u001b[0;32m      9\u001b[0m     \u001b[0mob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 10\u001b[1;33m     \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     11\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m             \u001b[0mob\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m','\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'items'"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "\n",
    "\n",
    "def convert(x):\n",
    "    ''' Convert a json string to a flat python dictionary\n",
    "    which can be passed into Pandas. '''\n",
    "    ob = json.loads(x)\n",
    "    for k, v in ob.items():\n",
    "        if isinstance(v, list):\n",
    "            ob[k] = ','.join(str(v))\n",
    "        elif isinstance(v, dict):\n",
    "            for kk, vv in v.items():\n",
    "                ob['%s_%s' % (k, kk)] = vv\n",
    "            del ob[k]\n",
    "    return ob\n",
    "\n",
    "for json_filename in glob('*.json'):\n",
    "    csv_filename = '%s.csv' % json_filename[:-5]\n",
    "    print 'Converting %s to %s' % (json_filename, csv_filename)\n",
    "    df = pd.DataFrame([convert(line) for line in file(json_filename)])\n",
    "    df.to_csv(csv_filename, encoding='utf-8', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Container object of 5 artists>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYgAAAEACAYAAACpoOGTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nSV3/D1mN/e1GqmQ0AAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fdc10aac910>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "overallDF = df[[\"overall\",\"reviewText\"]]\n",
    "\n",
    "data = df.groupby(\"overall\")[\"overall\"].count()\n",
    "plt.bar(data.index.values,data.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cachedStopWords' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-d0bf04c40b9d>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcachedStopWords\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'cachedStopWords' is not defined"
     ]
    }
   ],
   "source": [
    "cachedStopWords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Hay que igualar y coger muestras de tama√±o similar para todas las valoraciones?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.-  Procesamiento de texto sobre las reviews\n",
    "Eliminamos stopwords (TF-idf)\n",
    "aplicamos m√©todos de lematizacion - TextBlob, NLTK\n",
    "extraemos entidades (tokens)\n",
    "\n",
    "## 3.- Extracci√≥n de muestras de test y de entrenamiento para el modelo\n",
    "## 4.- Entrenamiento del modelo a partir de los datos de test.\n",
    "## 5.- Ejecuci√≥n del modelo. Evaluaci√≥n de resultados, comparaci√≥n con otros modelos Sentiment Analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob\n",
    "from textblob import Word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download() hay que hacerlo la primera vez para cargar todos los corpus necesarios\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "WordList([u'I', u'highly', u'recommend', u'this', u'series', u'It', u'is', u'a', u'must', u'for', u'anyone', u'who', u'is', u'yearning', u'to', u'watch', u'grown', u'up', u'television', u'Complex', u'character', u'and', u'plot', u'to', u'keep', u'one', u'totally', u'involved', u'Thank', u'you', u'Amazin', u'Prime'])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TextBlob(overallDF[\"reviewText\"][1]).words.lemmatize()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## funcion que elimina las stopwords y aplica lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "cachedStopWords = stopwords.words(\"english\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'i',\n",
       " u'me',\n",
       " u'my',\n",
       " u'myself',\n",
       " u'we',\n",
       " u'our',\n",
       " u'ours',\n",
       " u'ourselves',\n",
       " u'you',\n",
       " u'your',\n",
       " u'yours',\n",
       " u'yourself',\n",
       " u'yourselves',\n",
       " u'he',\n",
       " u'him',\n",
       " u'his',\n",
       " u'himself',\n",
       " u'she',\n",
       " u'her',\n",
       " u'hers',\n",
       " u'herself',\n",
       " u'it',\n",
       " u'its',\n",
       " u'itself',\n",
       " u'they',\n",
       " u'them',\n",
       " u'their',\n",
       " u'theirs',\n",
       " u'themselves',\n",
       " u'what',\n",
       " u'which',\n",
       " u'who',\n",
       " u'whom',\n",
       " u'this',\n",
       " u'that',\n",
       " u'these',\n",
       " u'those',\n",
       " u'am',\n",
       " u'is',\n",
       " u'are',\n",
       " u'was',\n",
       " u'were',\n",
       " u'be',\n",
       " u'been',\n",
       " u'being',\n",
       " u'have',\n",
       " u'has',\n",
       " u'had',\n",
       " u'having',\n",
       " u'do',\n",
       " u'does',\n",
       " u'did',\n",
       " u'doing',\n",
       " u'a',\n",
       " u'an',\n",
       " u'the',\n",
       " u'and',\n",
       " u'but',\n",
       " u'if',\n",
       " u'or',\n",
       " u'because',\n",
       " u'as',\n",
       " u'until',\n",
       " u'while',\n",
       " u'of',\n",
       " u'at',\n",
       " u'by',\n",
       " u'for',\n",
       " u'with',\n",
       " u'about',\n",
       " u'against',\n",
       " u'between',\n",
       " u'into',\n",
       " u'through',\n",
       " u'during',\n",
       " u'before',\n",
       " u'after',\n",
       " u'above',\n",
       " u'below',\n",
       " u'to',\n",
       " u'from',\n",
       " u'up',\n",
       " u'down',\n",
       " u'in',\n",
       " u'out',\n",
       " u'on',\n",
       " u'off',\n",
       " u'over',\n",
       " u'under',\n",
       " u'again',\n",
       " u'further',\n",
       " u'then',\n",
       " u'once',\n",
       " u'here',\n",
       " u'there',\n",
       " u'when',\n",
       " u'where',\n",
       " u'why',\n",
       " u'how',\n",
       " u'all',\n",
       " u'any',\n",
       " u'both',\n",
       " u'each',\n",
       " u'few',\n",
       " u'more',\n",
       " u'most',\n",
       " u'other',\n",
       " u'some',\n",
       " u'such',\n",
       " u'no',\n",
       " u'nor',\n",
       " u'not',\n",
       " u'only',\n",
       " u'own',\n",
       " u'same',\n",
       " u'so',\n",
       " u'than',\n",
       " u'too',\n",
       " u'very',\n",
       " u's',\n",
       " u't',\n",
       " u'can',\n",
       " u'will',\n",
       " u'just',\n",
       " u'don',\n",
       " u'should',\n",
       " u'now',\n",
       " u'd',\n",
       " u'll',\n",
       " u'm',\n",
       " u'o',\n",
       " u're',\n",
       " u've',\n",
       " u'y',\n",
       " u'ain',\n",
       " u'aren',\n",
       " u'couldn',\n",
       " u'didn',\n",
       " u'doesn',\n",
       " u'hadn',\n",
       " u'hasn',\n",
       " u'haven',\n",
       " u'isn',\n",
       " u'ma',\n",
       " u'mightn',\n",
       " u'mustn',\n",
       " u'needn',\n",
       " u'shan',\n",
       " u'shouldn',\n",
       " u'wasn',\n",
       " u'weren',\n",
       " u'won',\n",
       " u'wouldn']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cachedStopWords\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python2.7/site-packages/ipykernel/__main__.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "pandas.core.frame.DataFrame"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "overallDFsample=overallDF[1:5]\n",
    "overallDFsample['reviewTextLemmatized']=overallDFsample.apply(lemmatizeReviewText,axis=1)\n",
    "type (overallDFsample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'overallDFsample' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-3d964370d722>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0moverallDFsample\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'overallDFsample' is not defined"
     ]
    }
   ],
   "source": [
    "overallDFsample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "babyDf = pd.DataFrame([convert(line) for line in file('reviews_Baby_5.json')])\n",
    "babyDf.to_csv(csv_filename, encoding='utf-8', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'review_clean'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-30-25c007272da7>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mbabyDf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'review_clean'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m109033\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1795\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1796\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1797\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1798\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1799\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1802\u001b[0m         \u001b[1;31m# get column\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1803\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1804\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1805\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1806\u001b[0m         \u001b[1;31m# duplicate columns & possible reduce dimensionaility\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[1;34m(self, item)\u001b[0m\n\u001b[0;32m   1082\u001b[0m         \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1083\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1084\u001b[1;33m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1085\u001b[0m             \u001b[0mres\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1086\u001b[0m             \u001b[0mcache\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mres\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, item, fastpath)\u001b[0m\n\u001b[0;32m   2849\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2850\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2851\u001b[1;33m                 \u001b[0mloc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2852\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2853\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/usr/lib64/python2.7/site-packages/pandas/core/index.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[1;34m(self, key, method)\u001b[0m\n\u001b[0;32m   1570\u001b[0m         \"\"\"\n\u001b[0;32m   1571\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1572\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_values_from_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1574\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3824)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:3704)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12280)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12231)\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'review_clean'"
     ]
    }
   ],
   "source": [
    "babyDf['review_clean'][109033]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         perfect new parent able keep track baby feedin...\n",
       "1         book life saver ha helpful able go back track ...\n",
       "2         help know exactly baby day ha gone mother law ...\n",
       "3         bought time older son bought newborn super eas...\n",
       "4         wanted alternative printing daily log sheet na...\n",
       "5         great basic wish space write thing wa bigger l...\n",
       "6         3 month old son spend half day mother half nei...\n",
       "7         book perfect im first time new mom book made e...\n",
       "8         wanted love wa pretty expensive month worth ca...\n",
       "9         baby tracker brand book absolute best tracker ...\n",
       "10        postpartum stay hospital nurse ask keep log ba...\n",
       "11        use babysitter grandma keep track go day weve ...\n",
       "12        book great way keeping track daily changing fe...\n",
       "13        ha column info need glance get home work nanny...\n",
       "14        like log think would work better clearer pm se...\n",
       "15        wife six month old baby boy around 4month mark...\n",
       "16        thought keeping simple handwritten journal wou...\n",
       "17        easy use simple got baby wa 2 month old love m...\n",
       "18        used help u keep track pee poop first month re...\n",
       "19        item wa extremely helpful especially 3 adult s...\n",
       "20        ive using baby tracker since day daughter came...\n",
       "21        course ha greateasy way record baby routine si...\n",
       "22        ive using since day baby wa born 1 month ago l...\n",
       "23        didnt think would really use wa wrongin hospit...\n",
       "24        used book since son wa born great handy way ke...\n",
       "25        4 week delivery first baby wa looking newborn ...\n",
       "26        used tracker 3 child swear super easy leave ba...\n",
       "27        found book rummage sale found useful especiall...\n",
       "28        great journal use everyday communicate feeding...\n",
       "29        using baby tracker three month ha useful littl...\n",
       "                                ...                        \n",
       "160762    daughter excited finally facing forward first ...\n",
       "160763    short story wa disappointed quality thought en...\n",
       "160764    using graco car seat year good better others l...\n",
       "160765    argo 3in1 seat pretty much identical graco nau...\n",
       "160766    sturdy wellmade forwardfacing car seat child 2...\n",
       "160767    shrink wash terribly expected blue tread aroun...\n",
       "160768    love best bottom cloth diaper system cover sta...\n",
       "160769    go insert used best bottom diaper favorite use...\n",
       "160770    received 3 set 3 pack 9 total seam popped firs...\n",
       "160771    stay dry cotton large 24 month old amazing sup...\n",
       "160772    ive using best bottom month love product overa...\n",
       "160773    super absorbent incredibly soft even washed us...\n",
       "160774    love insert son heavy wetter super absorbent l...\n",
       "160775    im pretty new cloth diaper best bottom recomme...\n",
       "160776    insert work alright son heavy wetter dont work...\n",
       "160777    cheap therefor prefer econobums however son go...\n",
       "160778    ordered great success two package liner discov...\n",
       "160779    great insert im always impressed much hold esp...\n",
       "160780    quality isnt great honest stiching came apart ...\n",
       "160781    favorite insert favorite diaper easy clean car...\n",
       "160782    really absorbent super soft serging began fray...\n",
       "160783    ive used variety cloth cover insert favorite f...\n",
       "160784    best bottom system really great especially bab...\n",
       "160785    insert super absorbent little bulky though ove...\n",
       "160786    concept insert great keep moisture away baby s...\n",
       "160787    bought baby gift friend heard great thing look...\n",
       "160788    new cloth diapering wa leery thin insert would...\n",
       "160789    friend planning cloth diapering know need lot ...\n",
       "160790    love organic cottonhemp insert theyre super tr...\n",
       "160791    great bought hemp insert beginning dont stink ...\n",
       "Name: review_clean, dtype: object"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "def remove_punctuation(text):    \n",
    "    return text.translate(None, string.punctuation).lower()\n",
    "\n",
    "def lemmatizeReviewText(row):\n",
    "    words = TextBlob(row[\"review_clean\"]).words.lemmatize()\n",
    "    ext = ' '.join(word for word in words if word not in (cachedStopWords))\n",
    "    return ext\n",
    "\n",
    "# forzamos la columna reviewText como str para el correcto funcionamiento de remove_punctuation\n",
    "babyDf = pd.DataFrame([convert(line) for line in file('reviews_Baby_5.json')])\n",
    "babyDf['reviewText'] = babyDf['reviewText'].astype(str)\n",
    "babyDf['review_clean'] = babyDf['reviewText'].apply(remove_punctuation)\n",
    "babyDf['review_clean']= babyDf.apply(lemmatizeReviewText, axis=1)\n",
    "babyDf['review_clean']=babyDf['review_clean'].apply(lambda x: x.lower())\n",
    "\n",
    "babyDf.to_csv('reviews_Baby_5.csv', header=True,quoting=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Tratamos el dataset de Pets\n",
    "petsDf = pd.DataFrame([convert(line) for line in file('reviews_Pet_Supplies_5.json')])\n",
    "# petsDf.to_csv(csv_filename, encoding='utf-8', index=False)\n",
    "# forzamos la columna reviewText como str para el correcto funcionamiento de remove_punctuation\n",
    "petsDf['reviewText'] = petsDf['reviewText'].astype(str)\n",
    "petsDf['review_clean'] = petsDf['reviewText'].apply(remove_punctuation)\n",
    "petsDf['review_clean']= petsDf.apply(lemmatizeReviewText, axis=1)\n",
    "petsDf['review_clean']= petsDf['review_clean'].apply(lambda x: x.lower())\n",
    "petsDf.to_csv('reviews_Pet_Supplies_5.csv', header=True,quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318628"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concatenamos el dataset a partir de Pets y Babies\n",
    "\n",
    "frames = [babyDf,petsDf]\n",
    "\n",
    "babiesPetsDf = pd.concat(frames)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'babiesPetsDf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-1b541d61fb67>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mbabiesPetsDf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'babies+pets_reviews.csv'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mquoting\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'babiesPetsDf' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "babiesPetsDf.to_csv('babies+pets_reviews.csv', header=True,quoting=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "268599    nothing great thing say blue buffalo product r...\n",
       "110798    attend work conference thought would purchase ...\n",
       "218728    love company quality product excellent custome...\n",
       "220497    competitor product wa dark color crumbly made ...\n",
       "314210    easy install look great color matching highlan...\n",
       "66635     nice soft tie easily gentle teething son gum p...\n",
       "73390     product worth super sturdy imagine going last ...\n",
       "186168    isnt favorite good variety diet girl like pate...\n",
       "191465    ok dog like favorite work toy use treat since ...\n",
       "284166    beautiful vivid color sturdy even withheld 4 m...\n",
       "Name: review_clean, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comprobamos que est√° bien generado el csv volviendolo a cargar en el DF\n",
    "import json\n",
    "import pandas as pd\n",
    "babiesPetsDf=pd.read_csv('babies+pets_reviews.csv',header=False,sep=',')\n",
    "babiesPetsDf['review_clean'].sample(10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#  crear un diccionario {palabra:ocurrencias}\n",
    "import numpy as np\n",
    "cnt = {}\n",
    "for linea in babiesPetsDf['review_clean'].astype(str).values:\n",
    "    for word in linea.split():\n",
    "        if (word not in cnt):\n",
    "            cnt[word] = 1\n",
    "        else:\n",
    "            cnt[word] += 1\n",
    "            \n",
    "wordsSerie=pd.Series(cnt,index=cnt.keys())\n",
    "wordsSerie=wordsSerie[wordsSerie.values>1]\n",
    "data={'a':wordsSerie.index,'b':wordsSerie.values}\n",
    "wordsDf=pd.DataFrame(data=data, index=np.arange(len(wordsSerie)))\n",
    "wordsDf=wordsDf.sort('b', ascending=False).head(3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>a</th>\n",
       "      <th>b</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>73974</th>\n",
       "      <td>wa</td>\n",
       "      <td>181452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45914</th>\n",
       "      <td>one</td>\n",
       "      <td>159613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78246</th>\n",
       "      <td>dog</td>\n",
       "      <td>135114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3958</th>\n",
       "      <td>like</td>\n",
       "      <td>130835</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27250</th>\n",
       "      <td>love</td>\n",
       "      <td>115680</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65879</th>\n",
       "      <td>baby</td>\n",
       "      <td>107178</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3199</th>\n",
       "      <td>get</td>\n",
       "      <td>105242</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>68819</th>\n",
       "      <td>would</td>\n",
       "      <td>100830</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75180</th>\n",
       "      <td>use</td>\n",
       "      <td>98451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29521</th>\n",
       "      <td>ha</td>\n",
       "      <td>94032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17414</th>\n",
       "      <td>great</td>\n",
       "      <td>93335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12603</th>\n",
       "      <td>cat</td>\n",
       "      <td>93298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59758</th>\n",
       "      <td>time</td>\n",
       "      <td>78878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35491</th>\n",
       "      <td>little</td>\n",
       "      <td>74632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13490</th>\n",
       "      <td>well</td>\n",
       "      <td>74554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60223</th>\n",
       "      <td>really</td>\n",
       "      <td>70393</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53827</th>\n",
       "      <td>product</td>\n",
       "      <td>69365</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79481</th>\n",
       "      <td>toy</td>\n",
       "      <td>67763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22453</th>\n",
       "      <td>also</td>\n",
       "      <td>64857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60822</th>\n",
       "      <td>work</td>\n",
       "      <td>64537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84234</th>\n",
       "      <td>good</td>\n",
       "      <td>62667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14429</th>\n",
       "      <td>dont</td>\n",
       "      <td>62573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43351</th>\n",
       "      <td>much</td>\n",
       "      <td>62407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23613</th>\n",
       "      <td>month</td>\n",
       "      <td>61633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7477</th>\n",
       "      <td>food</td>\n",
       "      <td>60248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27136</th>\n",
       "      <td>easy</td>\n",
       "      <td>59175</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78321</th>\n",
       "      <td>make</td>\n",
       "      <td>52600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50698</th>\n",
       "      <td>put</td>\n",
       "      <td>49792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81003</th>\n",
       "      <td>old</td>\n",
       "      <td>49662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19046</th>\n",
       "      <td>even</td>\n",
       "      <td>48630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36976</th>\n",
       "      <td>footprint</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>60407</th>\n",
       "      <td>labrador</td>\n",
       "      <td>492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50299</th>\n",
       "      <td>rubbery</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41098</th>\n",
       "      <td>airline</td>\n",
       "      <td>491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34243</th>\n",
       "      <td>zukes</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34654</th>\n",
       "      <td>kidney</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9155</th>\n",
       "      <td>gusset</td>\n",
       "      <td>490</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66125</th>\n",
       "      <td>repeat</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66636</th>\n",
       "      <td>steady</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73999</th>\n",
       "      <td>pouring</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19985</th>\n",
       "      <td>phase</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18698</th>\n",
       "      <td>youtube</td>\n",
       "      <td>489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63952</th>\n",
       "      <td>driver</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78878</th>\n",
       "      <td>harm</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>62374</th>\n",
       "      <td>die</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14752</th>\n",
       "      <td>lamp</td>\n",
       "      <td>488</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83402</th>\n",
       "      <td>gluten</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41088</th>\n",
       "      <td>aussie</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25559</th>\n",
       "      <td>debris</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47039</th>\n",
       "      <td>joke</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>806</th>\n",
       "      <td>gadget</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>689</th>\n",
       "      <td>maximum</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12241</th>\n",
       "      <td>bike</td>\n",
       "      <td>487</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52932</th>\n",
       "      <td>wound</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15434</th>\n",
       "      <td>russell</td>\n",
       "      <td>486</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55812</th>\n",
       "      <td>muslin</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67888</th>\n",
       "      <td>group</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35723</th>\n",
       "      <td>wiped</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27726</th>\n",
       "      <td>closely</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76541</th>\n",
       "      <td>fail</td>\n",
       "      <td>485</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3000 rows √ó 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               a       b\n",
       "73974         wa  181452\n",
       "45914        one  159613\n",
       "78246        dog  135114\n",
       "3958        like  130835\n",
       "27250       love  115680\n",
       "65879       baby  107178\n",
       "3199         get  105242\n",
       "68819      would  100830\n",
       "75180        use   98451\n",
       "29521         ha   94032\n",
       "17414      great   93335\n",
       "12603        cat   93298\n",
       "59758       time   78878\n",
       "35491     little   74632\n",
       "13490       well   74554\n",
       "60223     really   70393\n",
       "53827    product   69365\n",
       "79481        toy   67763\n",
       "22453       also   64857\n",
       "60822       work   64537\n",
       "84234       good   62667\n",
       "14429       dont   62573\n",
       "43351       much   62407\n",
       "23613      month   61633\n",
       "7477        food   60248\n",
       "27136       easy   59175\n",
       "78321       make   52600\n",
       "50698        put   49792\n",
       "81003        old   49662\n",
       "19046       even   48630\n",
       "...          ...     ...\n",
       "36976  footprint     492\n",
       "60407   labrador     492\n",
       "50299    rubbery     491\n",
       "41098    airline     491\n",
       "34243      zukes     490\n",
       "34654     kidney     490\n",
       "9155      gusset     490\n",
       "66125     repeat     489\n",
       "66636     steady     489\n",
       "73999    pouring     489\n",
       "19985      phase     489\n",
       "18698    youtube     489\n",
       "63952     driver     488\n",
       "78878       harm     488\n",
       "62374        die     488\n",
       "14752       lamp     488\n",
       "83402     gluten     487\n",
       "41088     aussie     487\n",
       "25559     debris     487\n",
       "47039       joke     487\n",
       "806       gadget     487\n",
       "689      maximum     487\n",
       "12241       bike     487\n",
       "52932      wound     486\n",
       "15434    russell     486\n",
       "55812     muslin     485\n",
       "67888      group     485\n",
       "35723      wiped     485\n",
       "27726    closely     485\n",
       "76541       fail     485\n",
       "\n",
       "[3000 rows x 2 columns]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wordsDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "important_words = str([str(s) for s in wordsDf['a']])\n",
    "f= open('important_words_3000.json', 'w') \n",
    "f.write(important_words.replace(\"'\",'\"'))\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
