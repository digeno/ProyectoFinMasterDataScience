{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model: Predicting sentiment from product reviews\n",
    "\n",
    "\n",
    "The goal of this first notebook is to explore logistic regression and feature engineering with a model created from scratch.\n",
    "\n",
    "In this notebook we will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative.\n",
    "## Fire up [Sframe](https://github.com/dato-code/SFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "amazon_reviews.csv es la unión de varios datasets (ver notebook Data Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-06-09 08:23:11,103 [INFO] sframe.cython.cy_server, 172: SFrame v1.9 started. Logging /tmp/sframe_server_1465453390.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 55493 lines. Lines per second: 46679.4</pre>"
      ],
      "text/plain": [
       "Read 55493 lines. Lines per second: 46679.4"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"121431\",\"121431\",\"B0050O92FU\",\"[,0,,, ,0,]\",\"We purchased our Goodbyn lunchboxes at a local retail store recently and the kids have officially started using them. I find that the deep compartments are great to fill with snacks and different variety for sc...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"121431\",\"121431\",\"B0050O92FU\",\"[,0,,, ,0,]\",\"We purchased our Goodbyn lunchboxes at a local retail store recently and the kids have officially started using them. I find that the deep compartments are great to fill with snacks and different variety for sc...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"203556\",\"42764\",\"B000FP68KQ\",\"[,0,,, ,0,]\",\"I love this item, very easy to go on easy to use harness clip and she loves to wear it\",\"05 3, 2014\",\"A3IPRBCQK9JWD0\",\"Mz. B$$ \"\"L. Bennett\"\"\",\"harness dress\\\",\"1399075200\",\"i love item easy go easy use harness ...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"203556\",\"42764\",\"B000FP68KQ\",\"[,0,,, ,0,]\",\"I love this item, very easy to go on easy to use harness clip and she loves to wear it\",\"05 3, 2014\",\"A3IPRBCQK9JWD0\",\"Mz. B$$ \"\"L. Bennett\"\"\",\"harness dress\\\",\"1399075200\",\"i love item easy go easy use harness ...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"273479\",\"112687\",\"B0040QOYZ2\",\"[,0,,, ,0,]\",\"I have a Shiba Inu & she sheds SoO much. I bought the Furminator shampoo with the medium long hair comb. When I brush her it's like a razor blade edging her hair off on the top & stripping it. The hair that com...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"273479\",\"112687\",\"B0040QOYZ2\",\"[,0,,, ,0,]\",\"I have a Shiba Inu & she sheds SoO much. I bought the Furminator shampoo with the medium long hair comb. When I brush her it's like a razor blade edging her hair off on the top & stripping it. The hair that com...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"284660\",\"123868\",\"B004WSBZAA\",\"[,1,,, ,3,]\",\"I disliked the packaging and lack of information on the product itself and had to go hunting around the internet to find more information on how to administer/feed it to my dog. Powders are generally inconvenie...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"284660\",\"123868\",\"B004WSBZAA\",\"[,1,,, ,3,]\",\"I disliked the packaging and lack of information on the product itself and had to go hunting around the internet to find more information on how to administer/feed it to my dog. Powders are generally inconvenie...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>4 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "4 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/digeno/Documents/Cursos/BigData/Kschool-Data-Scientist/proyecto/git/babies+pets_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/digeno/Documents/Cursos/BigData/Kschool-Data-Scientist/proyecto/git/babies+pets_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 318624 lines in 4.5768 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 318624 lines in 4.5768 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[int,int,str,list,str,str,str,str,str,int,str,float]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "reviews = sframe.SFrame('babies+pets_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 1/6\n",
    "Let us quickly explore more of this dataset.\n",
    "1. We count the number of positive and negative reviews \n",
    "2. list the first 10 products in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318624"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+-------------------------------+\n",
      "| X1 | Unnamed: 0 |    asin    |            helpful            |\n",
      "+----+------------+------------+-------------------------------+\n",
      "| 0  |     0      | 097293751X | [None, 0, None, None, None... |\n",
      "| 1  |     1      | 097293751X | [None, 0, None, None, None... |\n",
      "+----+------------+------------+-------------------------------+\n",
      "+-------------------------------+-------------+----------------+\n",
      "|           reviewText          |  reviewTime |   reviewerID   |\n",
      "+-------------------------------+-------------+----------------+\n",
      "| Perfect for new parents. W... | 07 16, 2013 | A1HK2FQW6KXQB2 |\n",
      "| This book is such a life s... | 06 29, 2013 | A19K65VY14D13R |\n",
      "+-------------------------------+-------------+----------------+\n",
      "+-------------------------------+-------------------------------+----------------+-----+\n",
      "|          reviewerName         |            summary            | unixReviewTime | ... |\n",
      "+-------------------------------+-------------------------------+----------------+-----+\n",
      "| Amanda Johnsen \"Amanda E. ... |            Awesine            |   1373932800   | ... |\n",
      "|             angela            | Should be required for all... |   1372464000   | ... |\n",
      "+-------------------------------+-------------------------------+----------------+-----+\n",
      "[318624 rows x 12 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.print_rows(num_rows=2, num_columns=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering: defining reviews with positive or negative sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll call data engineering, just defining what is a positive and negative sentiment. So let's do that right now. So in the subsection we're gonna define what's a positive and a negative sentiment.\n",
    "And so I'm gonna make an arbitrary choice here:\n",
    "1. Let's say that things that 4, 5 stars are things that people liked. So those are positives. \n",
    "2. Things that 1 and 2 stars are negative. \n",
    "3. ignore all 3 star reviews.\n",
    "So I'm gonna say a positive sentiment equals 4 star or 5 star reviews. So let's go ahead and add a new column to our table that defines the actual sentiment. So products new column called sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will **ignore** all reviews with *rating = 3*, since they tend to have a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285437"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = reviews[reviews['review_overall'] != 3]\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will assign reviews with a rating of 4 or higher to be *positive* reviews, while the ones with rating of 2 or lower are *negative*. For the sentiment column, we use +1 for the positive class label and -1 for the negative class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+-------------------------------+\n",
      "| X1 | Unnamed: 0 |    asin    |            helpful            |\n",
      "+----+------------+------------+-------------------------------+\n",
      "| 0  |     0      | 097293751X | [None, 0, None, None, None... |\n",
      "| 1  |     1      | 097293751X | [None, 0, None, None, None... |\n",
      "+----+------------+------------+-------------------------------+\n",
      "+-------------------------------+-------------+----------------+\n",
      "|           reviewText          |  reviewTime |   reviewerID   |\n",
      "+-------------------------------+-------------+----------------+\n",
      "| Perfect for new parents. W... | 07 16, 2013 | A1HK2FQW6KXQB2 |\n",
      "| This book is such a life s... | 06 29, 2013 | A19K65VY14D13R |\n",
      "+-------------------------------+-------------+----------------+\n",
      "+-------------------------------+-------------------------------+----------------+-----+\n",
      "|          reviewerName         |            summary            | unixReviewTime | ... |\n",
      "+-------------------------------+-------------------------------+----------------+-----+\n",
      "| Amanda Johnsen \"Amanda E. ... |            Awesine            |   1373932800   | ... |\n",
      "|             angela            | Should be required for all... |   1372464000   | ... |\n",
      "+-------------------------------+-------------------------------+----------------+-----+\n",
      "[285437 rows x 13 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews['review_sentiment'] = reviews['review_overall'].apply(lambda rating : +1 if rating > 3 else -1)\n",
    "reviews.print_rows(num_rows=2, num_columns=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample dataset to make sure classes are balanced\n",
    "Just as we did in the previous assignment, we will undersample the larger class (safe loans) in order to balance out our dataset. This means we are throwing away many data points. We use `seed=1` so everyone gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive reviews             : 0.50116556825\n",
      "Percentage of negative reviews             : 0.49883443175\n",
      "Total number of reviews in our new dataset : 69494\n"
     ]
    }
   ],
   "source": [
    "positive_reviews_raw = reviews[reviews['review_sentiment'] == 1]\n",
    "negative_reviews_raw  = reviews[reviews['review_sentiment'] == -1]\n",
    "\n",
    "# Undersample the reviews.\n",
    "percentage = len(negative_reviews_raw)/float(len(positive_reviews_raw))\n",
    "negative_reviews = negative_reviews_raw\n",
    "positive_reviews = positive_reviews_raw.sample(percentage, seed=1)\n",
    "products = negative_reviews_raw.append(positive_reviews)\n",
    "\n",
    "print \"Percentage of positive reviews             :\", len(positive_reviews) / float(len(products))\n",
    "print \"Percentage of negative reviews             :\", len(negative_reviews) / float(len(products))\n",
    "print \"Total number of reviews in our new dataset :\", len(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 2/6\n",
    "Let us quickly explore more of this dataset.\n",
    "1. We count the number of positive and negative reviews.\n",
    "2. Modify the subset to contain similar numbers of positive and negative reviews, as the original dataset consisted primarily of positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of positive reviews = 250771\n",
      "# of negative reviews = 34666\n"
     ]
    }
   ],
   "source": [
    "print '# of positive reviews =', len(reviews[reviews['review_sentiment']==1])\n",
    "print '# of negative reviews =', len(reviews[reviews['review_sentiment']==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of positive balanced reviews = 34828\n",
      "# of negative balanced reviews = 34666\n"
     ]
    }
   ],
   "source": [
    "print '# of positive balanced reviews =', len(products[products['review_sentiment']==1])\n",
    "print '# of negative balanced reviews =', len(products[products['review_sentiment']==-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Extraction Phase: data preparation. \n",
    "## (desarrollado en el notebook Data Engineering)\n",
    "**Note:** \n",
    " - column review_clean with text cleaning developed in Data Engineering notebook\n",
    " - building features: la lista de palabras important_words.json se confecciona en el nb Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are several ways of doing this. We use the built-in *count* function for Python lists. Each **review without punctuation, stopwords, etc** string is first split into individual words and the number of occurances of a given word is counted.\n",
    "1. Transform the reviews into word-counts (only for **important_words**, without punctuation, stopwords, etc)\n",
    "2. For each word in **important_words**, we compute a count for the number of times the word occurs in the review. We will store this count in a separate column (one for each word). The result of this feature processing is a single column for each word in **important_words** which keeps a count of the number of times the respective word occurs in the review text.\n",
    "\n",
    "Now, we will load these words from this JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('important_words.json', 'r') as f: # Reads the list of words\n",
    "    important_words = json.load(f)\n",
    "important_words = [str(s) for s in important_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "193"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(important_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['baby', 'one', 'great', 'love', 'use', 'would', 'like', 'easy', 'little', 'seat', 'old', 'well', 'get', 'also', 'really', 'son', 'time', 'bought', 'product', 'good', 'daughter', 'much', 'loves', 'stroller', 'put', 'months', 'car', 'still', 'back', 'used', 'recommend', 'first', 'even', 'perfect', 'nice', 'bag', 'two', 'using', 'got', 'fit', 'around', 'diaper', 'enough', 'month', 'price', 'go', 'could', 'soft', 'since', 'buy', 'room', 'works', 'made', 'child', 'keep', 'size', 'small', 'need', 'year', 'big', 'make', 'take', 'easily', 'think', 'crib', 'clean', 'way', 'quality', 'thing', 'better', 'without', 'set', 'new', 'every', 'cute', 'best', 'bottles', 'work', 'purchased', 'right', 'lot', 'side', 'happy', 'comfortable', 'toy', 'able', 'kids', 'bit', 'night', 'long', 'fits', 'see', 'us', 'another', 'play', 'day', 'money', 'monitor', 'tried', 'thought', 'never', 'item', 'hard', 'plastic', 'however', 'disappointed', 'reviews', 'something', 'going', 'pump', 'bottle', 'cup', 'waste', 'return', 'amazon', 'different', 'top', 'want', 'problem', 'know', 'water', 'try', 'received', 'sure', 'times', 'chair', 'find', 'hold', 'gate', 'open', 'bottom', 'away', 'actually', 'cheap', 'worked', 'getting', 'ordered', 'came', 'milk', 'bad', 'part', 'worth', 'found', 'cover', 'many', 'design', 'looking', 'weeks', 'say', 'wanted', 'look', 'place', 'purchase', 'looks', 'second', 'piece', 'box', 'pretty', 'trying', 'difficult', 'together', 'though', 'give', 'started', 'anything', 'last', 'company', 'come', 'returned', 'maybe', 'took', 'broke', 'makes', 'stay', 'instead', 'idea', 'head', 'said', 'less', 'went', 'working', 'high', 'unit', 'seems', 'picture', 'completely', 'wish', 'buying', 'babies', 'won', 'tub', 'almost', 'either']\n"
     ]
    }
   ],
   "source": [
    "print important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 3/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SFrame **products** now contains one column for each of the **important_words**. As an example, the column **perfect** contains a count of the number of times the word **perfect** occurs in each of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype: int\n",
       "Rows: 69494\n",
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['perfect']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write some code to compute the number of product reviews that contain the word **perfect**.\n",
    "* First create a column called `contains_perfect` which is set to 1 if the count of the word **perfect** (stored in column **perfect**) is >= 1.\n",
    "* Sum the number of 1s in the column `contains_perfect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products['contains_perfect'] = products['perfect'].apply(lambda s : +1 if s >= 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3516"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['contains_perfect'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing logistic regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## link function (estimating conditional probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lecture that the link function is given by:\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))},\n",
    "$$\n",
    "\n",
    "where the feature vector $h(\\mathbf{x}_i)$ represents the word counts of **important_words** in the review  $\\mathbf{x}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "produces probablistic estimate for P(y_i = +1 | x_i, w).\n",
    "estimate ranges between 0 and 1.\n",
    "'''\n",
    "\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    predictions = 1.0 / (1.0 + np.exp(-scores))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the link function works with matrix algebra?\n",
    "\n",
    "Since the word counts are stored as columns in **feature_matrix**, each $i$-th row of the matrix corresponds to the feature vector $h(\\mathbf{x}_i)$:\n",
    "$$\n",
    "[\\text{feature_matrix}] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T \\\\\n",
    "h(\\mathbf{x}_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "h_0(\\mathbf{x}_1) & h_1(\\mathbf{x}_1) & \\cdots & h_D(\\mathbf{x}_1) \\\\\n",
    "h_0(\\mathbf{x}_2) & h_1(\\mathbf{x}_2) & \\cdots & h_D(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_0(\\mathbf{x}_N) & h_1(\\mathbf{x}_N) & \\cdots & h_D(\\mathbf{x}_N)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "By the rules of matrix multiplication, the score vector containing elements $\\mathbf{w}^T h(\\mathbf{x}_i)$ is obtained by multiplying **feature_matrix** and the coefficient vector $\\mathbf{w}$.\n",
    "$$\n",
    "[\\text{score}] =\n",
    "[\\text{feature_matrix}]\\mathbf{w} =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T \\\\\n",
    "h(\\mathbf{x}_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\mathbf{w}\n",
    "= \\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T\\mathbf{w} \\\\\n",
    "h(\\mathbf{x}_2)^T\\mathbf{w} \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\\mathbf{w}\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\left[\n",
    "\\begin{array}{c}\n",
    "\\mathbf{w}^T h(\\mathbf{x}_1) \\\\\n",
    "\\mathbf{w}^T h(\\mathbf{x}_2) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{w}^T h(\\mathbf{x}_N)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute derivative of log likelihood with respect to a single coefficient\n",
    "\n",
    "Recall:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "Function that computes the derivative of log likelihood with respect to a single coefficient $w_j$. The function accepts two arguments:\n",
    "* `errors` vector containing $\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})$ for all $i$.\n",
    "* `feature` vector containing $h_j(\\mathbf{x}_i)$  for all $i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):     \n",
    "    # Compute the dot product of errors and feature\n",
    "    derivative = np.dot(errors, feature)\n",
    "    \n",
    "    # Return the derivative\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduced a transformation of this likelihood---called the log likelihood---that simplifies the derivation of the gradient and is more numerically stable.  Due to its numerical stability, we will use the log likelihood instead of the likelihood to assess the algorithm.\n",
    "\n",
    "The log likelihood is computed using the following formula (see the advanced optional video if you are curious about the derivation of this equation):\n",
    "\n",
    "$$\\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) $$\n",
    "\n",
    "Function to compute the log likelihood for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    logexp = np.log(1. + np.exp(-scores))\n",
    "    \n",
    "    # Simple check to prevent overflow\n",
    "    mask = np.isinf(logexp)\n",
    "    logexp[mask] = -scores[mask]\n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - logexp)\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking gradient steps\n",
    "Now we are ready to implement our own logistic regression. \n",
    "\n",
    "Function to solve the logistic regression model using gradient ascent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in xrange(max_iter):\n",
    "\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        for j in xrange(len(coefficients)): # loop over each coefficient\n",
    "            \n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            derivative = np.dot(errors,feature_matrix[:,j])\n",
    "            \n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            coefficients[j] = coefficients[j] + derivative*step_size\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, sentiment, coefficients)\n",
    "            print 'iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp)\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving a sentiment classifier with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets\n",
    "Let's perform a train/test split with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = products.random_split(.8, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "print '# of total reviews =', len(products)\n",
    "print '# of positive reviews on all data =', len(products[products['review_sentiment']==1])\n",
    "print '# of negative reviews on all data =', len(products[products['review_sentiment']==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "print '# of train_data reviews =', len(train_data)\n",
    "print '# of positive reviews on train data =', len(train_data[train_data['review_sentiment']==1])\n",
    "print '# of negative reviews on train data =', len(train_data[train_data['review_sentiment']==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "print '# of test_data reviews =', len(test_data)\n",
    "print '# of positive reviews on test data =', len(test_data[test_data['review_sentiment']==1])\n",
    "print '# of negative reviews on test data =', len(test_data[test_data['review_sentiment']==-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFrame to NumPy array\n",
    "NumPy is a powerful library for doing matrix manipulation. Let us convert our data to matrices and then implement our algorithms with matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that extracts columns from an SFrame and converts them into a NumPy array. Two arrays are returned: one representing features and another representing class labels. The feature matrix includes an additional column 'intercept' to take account of the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_numpy_data(data_sframe, features, label):\n",
    "    data_sframe['intercept'] = 1\n",
    "    features = ['intercept'] + features\n",
    "    features_sframe = data_sframe[features]\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    label_sarray = data_sframe[label]\n",
    "    label_array = label_sarray.to_numpy()\n",
    "    return(feature_matrix, label_array)\n",
    "\n",
    "def get_numpy_feature_matrix(data_sframe, features):\n",
    "    data_sframe['intercept'] = 1\n",
    "    features = ['intercept'] + features\n",
    "    features_sframe = data_sframe[features]\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    return(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert the train_data into NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "train_feature_matrix, train_sentiment = get_numpy_data(train_data, important_words, 'review_sentiment') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55656, 194)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the sentiment classifier on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -38571.44967856\n",
      "iteration   1: log likelihood of observed labels = -38565.10715373\n",
      "iteration   2: log likelihood of observed labels = -38558.77189383\n",
      "iteration   3: log likelihood of observed labels = -38552.44388596\n",
      "iteration   4: log likelihood of observed labels = -38546.12311728\n",
      "iteration   5: log likelihood of observed labels = -38539.80957503\n",
      "iteration   6: log likelihood of observed labels = -38533.50324648\n",
      "iteration   7: log likelihood of observed labels = -38527.20411895\n",
      "iteration   8: log likelihood of observed labels = -38520.91217981\n",
      "iteration   9: log likelihood of observed labels = -38514.62741651\n",
      "iteration  10: log likelihood of observed labels = -38508.34981653\n",
      "iteration  11: log likelihood of observed labels = -38502.07936738\n",
      "iteration  12: log likelihood of observed labels = -38495.81605666\n",
      "iteration  13: log likelihood of observed labels = -38489.55987199\n",
      "iteration  14: log likelihood of observed labels = -38483.31080105\n",
      "iteration  15: log likelihood of observed labels = -38477.06883157\n",
      "iteration  20: log likelihood of observed labels = -38445.96507963\n",
      "iteration  30: log likelihood of observed labels = -38384.28325695\n",
      "iteration  40: log likelihood of observed labels = -38323.29257934\n",
      "iteration  50: log likelihood of observed labels = -38262.98167304\n",
      "iteration  60: log likelihood of observed labels = -38203.33952035\n",
      "iteration  70: log likelihood of observed labels = -38144.35542622\n",
      "iteration  80: log likelihood of observed labels = -38086.01898983\n",
      "iteration  90: log likelihood of observed labels = -38028.32008038\n",
      "iteration 100: log likelihood of observed labels = -37971.24881645\n",
      "iteration 200: log likelihood of observed labels = -37433.03201151\n",
      "iteration 300: log likelihood of observed labels = -36948.10412862\n"
     ]
    }
   ],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "#Atención el número de ceros debe coincidir con el número de palabras más uno.\n",
    "sentiment_model_coefficients = logistic_regression(train_feature_matrix, train_sentiment, initial_coefficients=np.zeros(train_feature_matrix.shape[1]),\n",
    "                                   step_size=1e-7, max_iter=301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class predictions from scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class predictions for a data point $\\mathbf{x}$ can be computed from the coefficients $\\mathbf{w}$ using the following formula:\n",
    "$$\n",
    "\\hat{y}_i = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{x}_i^T\\mathbf{w} > 0 \\\\\n",
    "      -1 & \\mathbf{x}_i^T\\mathbf{w} \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Now, we will write some code to compute class predictions. We will do this in two steps:\n",
    "* **Step 1**: First compute the **scores** using **feature_matrix** and **coefficients** using a dot product.\n",
    "* **Step 2**: Using the formula above, compute the class predictions from the scores.\n",
    "\n",
    "Step 1 can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Compute the scores as a dot product between feature_matrix and coefficients.\n",
    "scores = np.dot(train_feature_matrix, sentiment_model_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: compute the class predictions using the **scores** obtained above:\n",
    "train_sentiment_predictions = map((lambda score: +1 if score > 0 else -1), scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy of the model\n",
    "\n",
    "We will now measure the classification accuracy of the model. \n",
    "In accuracy, instead of measuring the number of errors, we measure the number of correct classifications.\n",
    "So the ratio here is number of correct divided by total number of sentences. \n",
    "In terms of accuracy, the best possible value is 1, I've got all the sentences right. \n",
    "The classification accuracy can be computed as follows:\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified data points}}{\\mbox{# total data points}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "# Reviews   correctly classified = 40629\n",
      "# Reviews incorrectly classified = 15027\n",
      "# Reviews total                  = 55656\n",
      "-----------------------------------------------------\n",
      "Accuracy = 0.73\n"
     ]
    }
   ],
   "source": [
    "num_mistakes = (train_sentiment != train_sentiment_predictions).sum()\n",
    "accuracy = 1.0 * (len(train_data) - num_mistakes) / len(train_data)\n",
    "print \"-----------------------------------------------------\"\n",
    "print '# Reviews   correctly classified =', len(train_data) - num_mistakes\n",
    "print '# Reviews incorrectly classified =', num_mistakes\n",
    "print '# Reviews total                  =', len(train_data)\n",
    "print \"-----------------------------------------------------\"\n",
    "print 'Accuracy = %.2f' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 4/6\n",
    "## Which words contribute most to positive & negative sentiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to compute the \"**most positive words**\". These are words that correspond most strongly with positive reviews. In order to do this, we will first do the following:\n",
    "* Treat each coefficient as a tuple, i.e. (**word**, **coefficient_value**).\n",
    "* Sort all the (**word**, **coefficient_value**) tuples by **coefficient_value** in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_model_coefficients_without_intercept = list(sentiment_model_coefficients[1:]) # exclude intercept\n",
    "word_coefficient_tuples = [(word, coefficient) for word, coefficient in zip(important_words, sentiment_model_coefficients_without_intercept)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **word_coefficient_tuples** contains a sorted list of (**word**, **coefficient_value**) tuples. The first 10 elements in this list correspond to the words that are most positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twenty \"most positive\" words\n",
    "\n",
    "Now, we compute the 10 words that have the most positive coefficient values. These words are associated with positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 0.11277188122068116),\n",
       " ('great', 0.075938590332108202),\n",
       " ('easy', 0.057972201241614042),\n",
       " ('little', 0.038402066990676276),\n",
       " ('well', 0.02952694528102048),\n",
       " ('perfect', 0.026895034018075126),\n",
       " ('keep', 0.022347573054671492),\n",
       " ('nice', 0.020208087987987247),\n",
       " ('happy', 0.017744724952088415),\n",
       " ('best', 0.015755116757510412),\n",
       " ('size', 0.013990892081284153),\n",
       " ('clean', 0.01351935435241074),\n",
       " ('bit', 0.013200538430718101),\n",
       " ('soft', 0.01319152526594493),\n",
       " ('use', 0.013099779930908132),\n",
       " ('stroller', 0.012044910463464634),\n",
       " ('price', 0.010760012175339593),\n",
       " ('play', 0.010624717394704339),\n",
       " ('car', 0.010201951575633974),\n",
       " ('recommend', 0.0099526220015287315)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_tuples[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twenty \"most negative\" words\n",
    "\n",
    "Next, we repeat this exercise on the 10 most negative words.  That is, we compute the 10 words that have the most negative coefficient values. These words are associated with negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('would', -0.062292081024304335),\n",
       " ('product', -0.039253902479188807),\n",
       " ('money', -0.036058980312379717),\n",
       " ('even', -0.033987619088352843),\n",
       " ('waste', -0.026659028607144156),\n",
       " ('thought', -0.025361645641945756),\n",
       " ('back', -0.023427287061877432),\n",
       " ('work', -0.021915430304519083),\n",
       " ('tried', -0.02136540041181673),\n",
       " ('disappointed', -0.020998813307555497),\n",
       " ('could', -0.020255156000479425),\n",
       " ('return', -0.019967134911871329),\n",
       " ('better', -0.018951719989793211),\n",
       " ('maybe', -0.018752870326481711),\n",
       " ('plastic', -0.018369502127576886),\n",
       " ('way', -0.018219623981386688),\n",
       " ('first', -0.017030781095435179),\n",
       " ('made', -0.016933594094424596),\n",
       " ('get', -0.015656146037324035),\n",
       " ('hard', -0.015421092845829801)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_tuples[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set. Making predictions with logistic regression\n",
    "Now that a model is trained, we can make predictions on the **test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to convert test_data into the sparse matrix format first.\n",
    "# Warning: This may take a few minutes...\n",
    "test_feature_matrix, test_sentiment = get_numpy_data(test_data, important_words, 'review_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Compute the scores as a dot product between feature_matrix and coefficients.\n",
    "scores = np.dot(test_feature_matrix, sentiment_model_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: compute the class predictions using the **scores** obtained above:\n",
    "test_sentiment_predictions = map((lambda score: +1 if score > 0 else -1), scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "# Reviews   correctly classified = 10121\n",
      "# Reviews incorrectly classified = 3717\n",
      "# Reviews total                  = 13838\n",
      "-----------------------------------------------------\n",
      "Accuracy = 0.73\n"
     ]
    }
   ],
   "source": [
    "num_test_mistakes = (test_sentiment != test_sentiment_predictions).sum()\n",
    "accuracy = 1.0 * (len(test_data) - num_test_mistakes) / len(test_data)\n",
    "print \"-----------------------------------------------------\"\n",
    "print '# Reviews   correctly classified =', len(test_data) - num_test_mistakes\n",
    "print '# Reviews incorrectly classified =', num_test_mistakes\n",
    "print '# Reviews total                  =', len(test_data)\n",
    "print \"-----------------------------------------------------\"\n",
    "print 'Accuracy = %.2f' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 5/6\n",
    "## Applying the learned model to understand sentiment for reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['predicted_sentiment'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_data.sort('predicted_sentiment', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">predicted_sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review_sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review_overall</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">reviewText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.11932507601</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Many weeks of online<br>research and in-person ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.10008333563</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I am  dog trainer, and<br>bought this 3 years ago. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.855717887896</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I got these from toys r<br>us which is slightly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.833379794684</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I love this car seat and<br>so do my kids.  My ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.79674969194</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">As parents of two little<br>ones, I'd like to say we ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[5 rows x 4 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tpredicted_sentiment\tfloat\n",
       "\treview_sentiment\tint\n",
       "\treview_overall\tfloat\n",
       "\treviewText\tstr\n",
       "\n",
       "Rows: 5\n",
       "\n",
       "Data:\n",
       "+---------------------+------------------+----------------+\n",
       "| predicted_sentiment | review_sentiment | review_overall |\n",
       "+---------------------+------------------+----------------+\n",
       "|    1.11932507601    |        1         |      5.0       |\n",
       "|    1.10008333563    |        1         |      5.0       |\n",
       "|    0.855717887896   |        1         |      5.0       |\n",
       "|    0.833379794684   |        1         |      5.0       |\n",
       "|    0.79674969194    |        -1        |      2.0       |\n",
       "+---------------------+------------------+----------------+\n",
       "+-------------------------------+\n",
       "|           reviewText          |\n",
       "+-------------------------------+\n",
       "| Many weeks of online resea... |\n",
       "| I am  dog trainer, and bou... |\n",
       "| I got these from toys r us... |\n",
       "| I love this car seat and s... |\n",
       "| As parents of two little o... |\n",
       "+-------------------------------+\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[['predicted_sentiment','review_sentiment','review_overall','reviewText']][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Many weeks of online research and in-person \"test driving\" went into my decision to buy an UPPAbaby Cruz.  Like many first time parents, I originally purchased a travel system.  After using the travel system stroller for 6 months, and realizing my son was uncomfortable and didn\\'t want to sit in it, I had a better idea of what features my family needed.My son needed: a very upright seat and an unobstructed view of the world, a reclining seat for napping, and plush fabric that didn\\'t make him overheat.I needed: something compact, easy to load and unload, relatively light, with tons of storage for my diaper bag and shopping bags. I also wanted something easy to clean.I primarily run errands with my stroller, or take walks around the neighborhood or paved walking park. A smooth ride was important, but I don\\'t jog or go off roading very often, so I didn\\'t want a stroller with big honkin\\' wheels.  Something stylish was a definite plus as well.Pros:-This is a quality made stroller.  The fabric feels durable, it\\'s construction is solid.  Other family members have had UPPAbaby strollers for years and highly recommended the brand to me.-A joy to push.  I love that the handlebar is height adjustable, and has a more squared off shape.  The steering is incredible,  I can turn around easily in tiny elevators.  It\\'s also only 22\" wide so fitting through crowded store aisles is a breeze.  The ride is super smooth.-The fabric is very durable and cleans up easily.  I love that you can pop the whole seat off and throw it in the wash if you want.  When spot cleaning stains wipe right off, even on the harness straps.  I also love that you have the option of adding the UPPAbaby Seat Liner (sold separately) for added comfort and stain protection.-The storage basket is massive.-Very stylish.  I\\'ve gotten compliments, which is always a nice bonus.-My 6 month old son is very comfortable and stays in it for hours without protesting.  I believe this is because of the plush fabrics, the wide open view and the upright seat.-It folds flatter than my travel system stroller, but takes up about the same space lengthwise.-The seat is reversible so baby can face you or the world.Cons:-No cup holder or parent console.  I got the Britax Stroller Organizer to fix this and like it.Britax Stroller Organizer, Black-The seat does recline 180, but still bends at the knee, so it\\'s not a truly flat fold.  My son hasn\\'t had any problems napping in it this way though.-The harness is a little weird.  The side and crotch straps tighten great, but on the lowest shoulder position, and upper straps seem loose for smaller babies.  Granted, this isn\\'t a car seat and my child is more than secure, but you may want to try it out in person and see if it meets your needs.  They may have been designed to be intentionally loose so your child can reach forward.  Plenty of room to grow into.-Belly Bar and Snack Tray sold separately.-Lowering the heights of the shoulder straps is difficult.  You have to wedge the nylon straps through the gap of a metal ring and then reconnect it the same way on a loop at a new height.  This isn\\'t a procedure you have to do very often, but for a stroller in this price range I wish is was designed a little better.  You have to do a similar maneuver to get the seat off as well.-The seat must be in a forward facing position to fold.Information That I Wish I\\'d Known:-In many review videos I\\'d watched on the Cruz, the way the stroller folded was a bit misrepresented.  There was criticism that you had to get down on the ground to fold the stroller, because it rolled forward and flat on the ground when being folded.  The instruction manual suggests engaging the brake pedal prior to folding.  This way, the stroller doesn\\'t roll forward and remains upright.  If you extend the handle all the way it will also stand once folded.-When removing the stroller from my car, I unlatch the stroller lock first, then turn around to open it.  I haven\\'t had get down to fold or unfold my Cruz like I\\'ve read in other reviews.  I was concerned about the bright yellow fabric hitting dirty pavement, but it isn\\'t an issue.Overall I am very pleased with my purchase, and find myself walking more often with the baby because I enjoy using this stroller so much!EDIT:  6 months later and I still love this stroller, and am just as thrilled when I bought it.  I added some non-skid padding used under area rugs to the Britax Stroller Organizer to keep it from sliding on the handles.  I also added the UPPAbaby seat liner, which is super plush and catches a lot of stains.  Is it weird to really love a stroller?  Because I really love this thing!'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most positive review\n",
    "test_data[0]['reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_data.sort('predicted_sentiment', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">predicted_sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review_sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review_overall</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">reviewText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1.25039590656</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">So I read many reviews on<br>this product and watched ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1.00866573429</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">THIS BASSINET IS<br>OVERPRICED AND ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.942785461211</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">after i bought a top-<br>rated, well-reviewed gps ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.916776118981</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">My advice on the Bright<br>Starts Sugar Blossom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.819272059814</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">UPDATE 6/4/13 - I'm<br>keeping the Vtech rating ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[5 rows x 4 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tpredicted_sentiment\tfloat\n",
       "\treview_sentiment\tint\n",
       "\treview_overall\tfloat\n",
       "\treviewText\tstr\n",
       "\n",
       "Rows: 5\n",
       "\n",
       "Data:\n",
       "+---------------------+------------------+----------------+\n",
       "| predicted_sentiment | review_sentiment | review_overall |\n",
       "+---------------------+------------------+----------------+\n",
       "|    -1.25039590656   |        -1        |      1.0       |\n",
       "|    -1.00866573429   |        -1        |      2.0       |\n",
       "|   -0.942785461211   |        -1        |      1.0       |\n",
       "|   -0.916776118981   |        -1        |      1.0       |\n",
       "|   -0.819272059814   |        -1        |      1.0       |\n",
       "+---------------------+------------------+----------------+\n",
       "+-------------------------------+\n",
       "|           reviewText          |\n",
       "+-------------------------------+\n",
       "| So I read many reviews on ... |\n",
       "| THIS BASSINET IS OVERPRICE... |\n",
       "| after i bought a top-rated... |\n",
       "| My advice on the Bright St... |\n",
       "| UPDATE 6/4/13 - I'm keepin... |\n",
       "+-------------------------------+\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[['predicted_sentiment','review_sentiment','review_overall','reviewText']][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So I read many reviews on this product and watched youtube videos on it as well. Everyone said the same thing and shame on me for not following my gut feeling. I've been in the aquarium hobby for years and this was my first attempt at setting this up the way its supposed to be done according to the directions. First of all, everyone said it leaks. Well, it does, and from a few places. The hose input and output leaks, the bottom screw on access point leaks and overall this is a cheaply made crappy product. Now, am I saying it doesn't work? IDK, but its not working for me simply on the fact that I shouldn't have to completely seal and bond every point of this contraption and use hose clamps which still leak. The directions may as well be a pop up book with no words, unless  you can read some vague chineese symbols if the directions translate into English. It never says what appropriate size hose to use, I tried 1/2 inch and it fits fine, but still leaks. 3/8 seemed too tight. 5/8 too big. The suction cups that are supposed to hold the unit mounted to the tank aren't even designed for this unit. The suction cup plastic nipple molded into the unit that is supposed to go into the suction cup hole, for one isn't big enough, for two even if you did get the nipple in without breaking it off it won't be held by anything inside of it because there is no inner lip that catches the fatter part of the nipple to hold the suction cup in. Instead it looks like there needs to be a larger bracket or slot that the suction cup tip slides into a hole and then holds the unilt to the suction cup, but there is surely no bracket for that whatsoever so that deems the suctions cups useless. Just a P*SS POOR design on that end. As for the overall functionality, from a design aspect the concept seems cool and would probably work as others have mentioned, but what does your time cost you? I spend about 2 hours trying to get this thing hooked up and running which took about 30 minutes at most between the hoses and my filter etc, but then the next 1.5 hours it took me to &#34;ATTEMPT&#34; to get this thing to stop leaking. I tried teflon tape, no go. Then added a bit of plumbers dope, no go. Cleaned that off and aside from probably needing to silicone the hole thing shut for good, I stopped there. Then came the hose ends leaking. Every attempt to see how the  water was leaking and to get it to stop was failed. And trust me I've remodeled houses and I do everything from plumbing to electrical and this thing doesn't serve its purpose. This needs to have barbed tips to hold the hose tighter than not and should come with some sort of hose clamps or even really secure zip locks if and when the correct hose that &#34;they suggest&#34; is used properly, but oh yeah they didn't do that. I don't feel I should have to set up metal hose clamps on a product like this. Oh, I forgot to mention for the little while that this was running the impellers would only spin when the unit was in a horizontal position. The directions say to keep it in an upright position at all times, YEAH RIGHT!! Everytime you put it in an upright position the impellers stop spinning. On one of my leak fixing attempts I took the main inner shaft out and it cracked too. Its like having an eggshell inner tube, instead of a nice rigid plastic durable inner tube that should be able to last. The plastic impeller blades are crap too. Overall this was a huge waste of time and money and I don't care how many people had to &#34;JIMMY RIG&#34; this thing to get it to function properly, but right out of the box this thing was a lost cause. If you are that serious about diffusing CO2 then I suggest taking more time to figure out another way instead of this as you may spend less time fussing with it, or be a bit creative and find a way to mimic this simple design if it works so well and build your own unit. I'm pretty much done with this plastic hunk of cheaply made chineese junk. I'm sure if an american company were making this it would have some sort of better standard, at least I would hope so. I'm not going to bother with customer service or returning it etc because at this point what are they going to say? Well, you modified it so it voided the warranty! Well, duh, no matter who you are or what you are using this for you are going to have to modify it and tinker with its major major flaws of design, material, and functionality so basically your warranty is already void before you purchase this. I usually give every product a fair shake, but this one by far is a total bummer. I only gave it 1 star because it arrived and was in a new box. Whoo hoo!! That's about all you will cheer for with this one. Like I said, I'm either going to have to make my own unit or pretty much settle on a ceramic diffuser that only costs  a few bucks compared to this crap. I wouldn't have paid 3 dollars for this thing at a flea market if I knew this would be the case. I think Amazon needs to take this one off the website as it requires major mods and a waste of time. New products that boast what this one boasts should not need to be modified or re-engineered in any way or form and if consistent leaks were the issue it should be a simple call to replace a faulty unit as it was made faulty. Seriously, if this review hasn't changed your mind then be my guest and give yourself a headache and thin out your wallet a little because that's about all I can say you will accomplish with this one. I'll make my own!! Complete waste.....and probably an outstanding concept and design in what it's supposed to do &#34;if it worked&#34; and was made with better material and simple design details were some sort of concern to the people making this. This looks like some kid's science fair project that he was making out of his garage in plastic molds with zero quality control and product testing. but aside from that this thing could have come in a cracker jack box and been worth more. JUNK!! Amazon, stop selling this product. I love you guys, but seriously this one is a joke overall. I don't want to be vulgar, but I've had &#34;protective latex gear&#34;, if you will, last longer than this.\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most negative review\n",
    "test_data[0]['reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 6/6: \n",
    "# TODO: Applying the learned model to discover insights on twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Extraction Phase: data preparation. \n",
    "### (desarrollado en el notebook Data Engineering)\n",
    "**Note:** \n",
    " - column text_clean with text cleaning developed in Data Engineering notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"2794\",\"737941761129992192\",\"@ebbtideapp Tide in Point Au Fer, Louisiana 06/01/2016\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"2794\",\"737941761129992192\",\"@ebbtideapp Tide in Point Au Fer, Louisiana 06/01/2016\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"1440\",\"737930256863113216\",\"hey vsauce michael here\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"1440\",\"737930256863113216\",\"hey vsauce michael here\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"4114\",\"737948289509654529\",\"6:04am: sunrise\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"4114\",\"737948289509654529\",\"6:04am: sunrise\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"11\",\"737913129292730368\",\"holy shit\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"11\",\"737913129292730368\",\"holy shit\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \" Low  5:14am  1.2\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \" Low  5:14am  1.2\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"do y'all got games on your phone\",\"Wed Jun 01 08:54:17 +0000 2016\",\"40.86970401\",\"-73.89353064\",\"Bronx, NY\",\"hey vsauce michael yall got game phone\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"do y'all got games on your phone\",\"Wed Jun 01 08:54:17 +0000 2016\",\"40.86970401\",\"-73.89353064\",\"Bronx, NY\",\"hey vsauce michael yall got game phone\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"sunset will be at 8:45pm\",\"Wed Jun 01 10:05:57 +0000 2016\",\"38.35\",\"-81.63\",\"Charleston, WV\",\"604am sunrise sunset 845pm\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"sunset will be at 8:45pm\",\"Wed Jun 01 10:05:57 +0000 2016\",\"38.35\",\"-81.63\",\"Charleston, WV\",\"604am sunrise sunset 845pm\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"@thebottlemen @ Webster Hall https://t.co/9Gh0dnaEih\",\"Wed Jun 01 07:46:14 +0000 2016\",\"40.7319055\",\"-73.9896578\",\"Manhattan, NY\",\"holy shit thebottlemen webster hall http tco9gh0dnaeih\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"@thebottlemen @ Webster Hall https://t.co/9Gh0dnaEih\",\"Wed Jun 01 07:46:14 +0000 2016\",\"40.7319055\",\"-73.9896578\",\"Manhattan, NY\",\"holy shit thebottlemen webster hall http tco9gh0dnaeih\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"High 12:48pm  1.8\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"High 12:48pm  1.8\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \" Low  5:48pm  0.0\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \" Low  5:48pm  0.0\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"5518\",\"737958911735529472\",\"#Top3Apps for 'Clinton 45'\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"5518\",\"737958911735529472\",\"#Top3Apps for 'Clinton 45'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>924 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "924 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/digeno/Documents/Cursos/BigData/Kschool-Data-Scientist/proyecto/git/us_tweets.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/digeno/Documents/Cursos/BigData/Kschool-Data-Scientist/proyecto/git/us_tweets.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.056218 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.056218 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"2794\",\"737941761129992192\",\"@ebbtideapp Tide in Point Au Fer, Louisiana 06/01/2016\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"2794\",\"737941761129992192\",\"@ebbtideapp Tide in Point Au Fer, Louisiana 06/01/2016\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"1440\",\"737930256863113216\",\"hey vsauce michael here\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"1440\",\"737930256863113216\",\"hey vsauce michael here\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"11\",\"737913129292730368\",\"holy shit\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"11\",\"737913129292730368\",\"holy shit\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"4114\",\"737948289509654529\",\"6:04am: sunrise\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"4114\",\"737948289509654529\",\"6:04am: sunrise\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \" Low  5:14am  1.2\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \" Low  5:14am  1.2\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"do y'all got games on your phone\",\"Wed Jun 01 08:54:17 +0000 2016\",\"40.86970401\",\"-73.89353064\",\"Bronx, NY\",\"hey vsauce michael yall got game phone\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"do y'all got games on your phone\",\"Wed Jun 01 08:54:17 +0000 2016\",\"40.86970401\",\"-73.89353064\",\"Bronx, NY\",\"hey vsauce michael yall got game phone\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"@thebottlemen @ Webster Hall https://t.co/9Gh0dnaEih\",\"Wed Jun 01 07:46:14 +0000 2016\",\"40.7319055\",\"-73.9896578\",\"Manhattan, NY\",\"holy shit thebottlemen webster hall http tco9gh0dnaeih\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"@thebottlemen @ Webster Hall https://t.co/9Gh0dnaEih\",\"Wed Jun 01 07:46:14 +0000 2016\",\"40.7319055\",\"-73.9896578\",\"Manhattan, NY\",\"holy shit thebottlemen webster hall http tco9gh0dnaeih\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"sunset will be at 8:45pm\",\"Wed Jun 01 10:05:57 +0000 2016\",\"38.35\",\"-81.63\",\"Charleston, WV\",\"604am sunrise sunset 845pm\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"sunset will be at 8:45pm\",\"Wed Jun 01 10:05:57 +0000 2016\",\"38.35\",\"-81.63\",\"Charleston, WV\",\"604am sunrise sunset 845pm\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"High 12:48pm  1.8\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"High 12:48pm  1.8\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"5518\",\"737958911735529472\",\"#Top3Apps for 'Clinton 45'\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"5518\",\"737958911735529472\",\"#Top3Apps for 'Clinton 45'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"6690\",\"737995754187423744\",\"#GlobalRunningDay #TheWalkingWalt \"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"6690\",\"737995754187423744\",\"#GlobalRunningDay #TheWalkingWalt \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \" Low  5:48pm  0.0\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \" Low  5:48pm  0.0\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>930 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "930 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/digeno/Documents/Cursos/BigData/Kschool-Data-Scientist/proyecto/git/us_tweets.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/digeno/Documents/Cursos/BigData/Kschool-Data-Scientist/proyecto/git/us_tweets.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5623 lines in 0.042654 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5623 lines in 0.042654 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[int,int,str,str,float,float,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tweets_data = sframe.SFrame('us_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5623"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tweets_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SFrame.column_names of Columns:\n",
       "\tX1\tint\n",
       "\ttweet_id\tint\n",
       "\ttweet_text\tstr\n",
       "\ttweet_created_at\tstr\n",
       "\ttweet_geo_lat\tfloat\n",
       "\ttweet_geo_lon\tfloat\n",
       "\ttweet_city\tstr\n",
       "\ttext_clean\tstr\n",
       "\n",
       "Rows: 5623\n",
       "\n",
       "Data:\n",
       "+----+--------------------+-------------------------------+\n",
       "| X1 |      tweet_id      |           tweet_text          |\n",
       "+----+--------------------+-------------------------------+\n",
       "| 0  | 737913052050423808 | Supplemental Health Care #... |\n",
       "| 1  | 737913053119959040 | Can you recommend anyone f... |\n",
       "| 2  | 737913055338713089 |    https://t.co/DZVgUmv8ZQ    |\n",
       "| 6  | 737913086837940226 | HCR108 [Passed] Requests t... |\n",
       "| 7  | 737913096543588352 | SB64 [Vetoed] Baltimore Co... |\n",
       "| 8  | 737913098217132032 | Just posted a video @ Rol-... |\n",
       "| 9  | 737913106752540672 | HB349 [Passed] Baltimore C... |\n",
       "| 12 | 737913131683446784 | Website Hacks to Trap more... |\n",
       "| 13 | 737913141993078784 | I'm at C-View Inn in Cape ... |\n",
       "| 14 | 737913142232162304 | geo e92dc4e56c6b97210b3383... |\n",
       "+----+--------------------+-------------------------------+\n",
       "+--------------------------------+---------------+---------------+\n",
       "|        tweet_created_at        | tweet_geo_lat | tweet_geo_lon |\n",
       "+--------------------------------+---------------+---------------+\n",
       "| Wed Jun 01 07:45:56 +0000 2016 |   37.8646149  |  -122.2341905 |\n",
       "| Wed Jun 01 07:45:56 +0000 2016 |   34.0522342  |  -118.2436849 |\n",
       "| Wed Jun 01 07:45:56 +0000 2016 |  37.69944509  | -123.01210475 |\n",
       "| Wed Jun 01 07:46:04 +0000 2016 |   30.456615   |   -91.187356  |\n",
       "| Wed Jun 01 07:46:06 +0000 2016 |   38.978862   |   -76.490685  |\n",
       "| Wed Jun 01 07:46:07 +0000 2016 |    25.88414   |   -80.24274   |\n",
       "| Wed Jun 01 07:46:09 +0000 2016 |   38.978862   |   -76.490685  |\n",
       "| Wed Jun 01 07:46:15 +0000 2016 |  47.07947453  | -120.00366211 |\n",
       "| Wed Jun 01 07:46:17 +0000 2016 |  38.94637803  |  -74.91018722 |\n",
       "| Wed Jun 01 07:46:17 +0000 2016 |   37.781157   |   -122.39872  |\n",
       "+--------------------------------+---------------+---------------+\n",
       "+-------------------+-------------------------------+\n",
       "|     tweet_city    |           text_clean          |\n",
       "+-------------------+-------------------------------+\n",
       "|    Oakland, CA    | supplemental health care h... |\n",
       "|  Los Angeles, CA  | can recommend anyone nursi... |\n",
       "|  California, USA  |       http tcodzvgumv8zq      |\n",
       "|  Baton Rouge, LA  | hcr108 passed requests dep... |\n",
       "|   Annapolis, MD   | sb64 vetoed baltimore coun... |\n",
       "|    Westview, FL   | just posted video rollexx ... |\n",
       "|   Annapolis, MD   | hb349 passed baltimore cou... |\n",
       "|  Washington, USA  | website hacks trap leads i... |\n",
       "|    Cape May, NJ   | i m cview inn cape may nj ... |\n",
       "| San Francisco, CA | geo e92dc4e56c6b97210b3383... |\n",
       "+-------------------+-------------------------------+\n",
       "[5623 rows x 8 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    tweets_data[word] = tweets_data['text_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to convert test_data into the sparse matrix format first.\n",
    "# Warning: This may take a few minutes...\n",
    "tweets_feature_matrix  = get_numpy_feature_matrix(tweets_data, important_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Compute the scores as a dot product between feature_matrix and coefficients.\n",
    "scores = np.dot(tweets_feature_matrix, sentiment_model_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_data['predicted_sentiment'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to json array to d3 visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_data = tweets_data.sort('predicted_sentiment', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">predicted_sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tweet_text</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tweet_geo_lat</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tweet_geo_lon</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tweet_city</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.23527106234</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">One cannot think well,<br>love well, sleep well ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">37.69894361</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-123.01177393</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">California, USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.145263538837</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Happy Birthday to the<br>best most beautiful sis ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">40.75224945</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-73.93153195</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Queens, NY</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[2 rows x 5 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tpredicted_sentiment\tfloat\n",
       "\ttweet_text\tstr\n",
       "\ttweet_geo_lat\tfloat\n",
       "\ttweet_geo_lon\tfloat\n",
       "\ttweet_city\tstr\n",
       "\n",
       "Rows: 2\n",
       "\n",
       "Data:\n",
       "+---------------------+-------------------------------+---------------+\n",
       "| predicted_sentiment |           tweet_text          | tweet_geo_lat |\n",
       "+---------------------+-------------------------------+---------------+\n",
       "|    0.23527106234    | One cannot think well, lov... |  37.69894361  |\n",
       "|    0.145263538837   | Happy Birthday to the best... |  40.75224945  |\n",
       "+---------------------+-------------------------------+---------------+\n",
       "+---------------+-----------------+\n",
       "| tweet_geo_lon |    tweet_city   |\n",
       "+---------------+-----------------+\n",
       "| -123.01177393 | California, USA |\n",
       "|  -73.93153195 |    Queens, NY   |\n",
       "+---------------+-----------------+\n",
       "[2 rows x 5 columns]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data[['predicted_sentiment','tweet_text','tweet_geo_lat','tweet_geo_lon','tweet_city']][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_tweets_data = tweets_data[['predicted_sentiment','tweet_text','tweet_geo_lat','tweet_geo_lon','tweet_city']][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_tweets_data.export_json('d3/data/USA-positive.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_data = tweets_data.sort('predicted_sentiment', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">predicted_sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tweet_text</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tweet_geo_lat</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tweet_geo_lon</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">tweet_city</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.120094571888</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">#InstaMag. And if you<br>want to donate towards ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">30.0945175</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-94.1181713</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Beaumont, TX</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.106761107288</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Work Work Work Work Work.<br>@ Mars Miami Recording ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">25.88477594</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-80.36834298</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Medley, FL</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[2 rows x 5 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tpredicted_sentiment\tfloat\n",
       "\ttweet_text\tstr\n",
       "\ttweet_geo_lat\tfloat\n",
       "\ttweet_geo_lon\tfloat\n",
       "\ttweet_city\tstr\n",
       "\n",
       "Rows: 2\n",
       "\n",
       "Data:\n",
       "+---------------------+-------------------------------+---------------+\n",
       "| predicted_sentiment |           tweet_text          | tweet_geo_lat |\n",
       "+---------------------+-------------------------------+---------------+\n",
       "|   -0.120094571888   | #InstaMag. And if you want... |   30.0945175  |\n",
       "|   -0.106761107288   | Work Work Work Work Work. ... |  25.88477594  |\n",
       "+---------------------+-------------------------------+---------------+\n",
       "+---------------+--------------+\n",
       "| tweet_geo_lon |  tweet_city  |\n",
       "+---------------+--------------+\n",
       "|  -94.1181713  | Beaumont, TX |\n",
       "|  -80.36834298 |  Medley, FL  |\n",
       "+---------------+--------------+\n",
       "[2 rows x 5 columns]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data[['predicted_sentiment','tweet_text','tweet_geo_lat','tweet_geo_lon','tweet_city']][0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_tweets_data = tweets_data[['predicted_sentiment','tweet_text','tweet_geo_lat','tweet_geo_lon','tweet_city']][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_tweets_data.export_json('d3/data/USA-negative.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most positives and negatives tweets of EEUU dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "def serve_html():\n",
    "    fn= './d3/html/tweets_map.html'\n",
    "    return IFrame(fn, 985, 570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "serve_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
