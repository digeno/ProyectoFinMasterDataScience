{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model: Predicting sentiment from product reviews\n",
    "\n",
    "\n",
    "The goal of this first notebook is to explore logistic regression and feature engineering with a model created from scratch.\n",
    "\n",
    "In this notebook we will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative.\n",
    "## Fire up [Sframe](https://github.com/dato-code/SFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "amazon_reviews.csv es la uni√≥n de varios datasets (ver notebook Data Engineering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2016-06-08 18:48:38,420 [INFO] sframe.cython.cy_server, 172: SFrame v1.9 started. Logging /tmp/sframe_server_1465404518.log\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre>Read 55493 lines. Lines per second: 46918.1</pre>"
      ],
      "text/plain": [
       "Read 55493 lines. Lines per second: 46918.1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"121431\",\"121431\",\"B0050O92FU\",\"[,0,,, ,0,]\",\"We purchased our Goodbyn lunchboxes at a local retail store recently and the kids have officially started using them. I find that the deep compartments are great to fill with snacks and different variety for sc...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"121431\",\"121431\",\"B0050O92FU\",\"[,0,,, ,0,]\",\"We purchased our Goodbyn lunchboxes at a local retail store recently and the kids have officially started using them. I find that the deep compartments are great to fill with snacks and different variety for sc...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"203556\",\"42764\",\"B000FP68KQ\",\"[,0,,, ,0,]\",\"I love this item, very easy to go on easy to use harness clip and she loves to wear it\",\"05 3, 2014\",\"A3IPRBCQK9JWD0\",\"Mz. B$$ \"\"L. Bennett\"\"\",\"harness dress\\\",\"1399075200\",\"i love item easy go easy use harness ...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"203556\",\"42764\",\"B000FP68KQ\",\"[,0,,, ,0,]\",\"I love this item, very easy to go on easy to use harness clip and she loves to wear it\",\"05 3, 2014\",\"A3IPRBCQK9JWD0\",\"Mz. B$$ \"\"L. Bennett\"\"\",\"harness dress\\\",\"1399075200\",\"i love item easy go easy use harness ...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"273479\",\"112687\",\"B0040QOYZ2\",\"[,0,,, ,0,]\",\"I have a Shiba Inu & she sheds SoO much. I bought the Furminator shampoo with the medium long hair comb. When I brush her it's like a razor blade edging her hair off on the top & stripping it. The hair that com...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"273479\",\"112687\",\"B0040QOYZ2\",\"[,0,,, ,0,]\",\"I have a Shiba Inu & she sheds SoO much. I bought the Furminator shampoo with the medium long hair comb. When I brush her it's like a razor blade edging her hair off on the top & stripping it. The hair that com...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"284660\",\"123868\",\"B004WSBZAA\",\"[,1,,, ,3,]\",\"I disliked the packaging and lack of information on the product itself and had to go hunting around the internet to find more information on how to administer/feed it to my dog. Powders are generally inconvenie...\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"284660\",\"123868\",\"B004WSBZAA\",\"[,1,,, ,3,]\",\"I disliked the packaging and lack of information on the product itself and had to go hunting around the internet to find more information on how to administer/feed it to my dog. Powders are generally inconvenie...\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>4 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "4 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/digeno/Documents/Cursos/BigData/Kschool-Data-Scientist/proyecto/git/babies+pets_reviews.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/digeno/Documents/Cursos/BigData/Kschool-Data-Scientist/proyecto/git/babies+pets_reviews.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 318624 lines in 4.52301 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 318624 lines in 4.52301 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[int,int,str,list,str,str,str,str,str,int,str,float]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "reviews = sframe.SFrame('babies+pets_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 1/6\n",
    "Let us quickly explore more of this dataset.\n",
    "1. We count the number of positive and negative reviews \n",
    "2. list the first 10 products in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "318624"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+-------------------------------+\n",
      "| X1 | Unnamed: 0 |    asin    |            helpful            |\n",
      "+----+------------+------------+-------------------------------+\n",
      "| 0  |     0      | 097293751X | [None, 0, None, None, None... |\n",
      "| 1  |     1      | 097293751X | [None, 0, None, None, None... |\n",
      "+----+------------+------------+-------------------------------+\n",
      "+-------------------------------+-------------+----------------+\n",
      "|           reviewText          |  reviewTime |   reviewerID   |\n",
      "+-------------------------------+-------------+----------------+\n",
      "| Perfect for new parents. W... | 07 16, 2013 | A1HK2FQW6KXQB2 |\n",
      "| This book is such a life s... | 06 29, 2013 | A19K65VY14D13R |\n",
      "+-------------------------------+-------------+----------------+\n",
      "+-------------------------------+-------------------------------+----------------+-----+\n",
      "|          reviewerName         |            summary            | unixReviewTime | ... |\n",
      "+-------------------------------+-------------------------------+----------------+-----+\n",
      "| Amanda Johnsen \"Amanda E. ... |            Awesine            |   1373932800   | ... |\n",
      "|             angela            | Should be required for all... |   1372464000   | ... |\n",
      "+-------------------------------+-------------------------------+----------------+-----+\n",
      "[318624 rows x 12 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews.print_rows(num_rows=2, num_columns=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering: defining reviews with positive or negative sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll call data engineering, just defining what is a positive and negative sentiment. So let's do that right now. So in the subsection we're gonna define what's a positive and a negative sentiment.\n",
    "And so I'm gonna make an arbitrary choice here:\n",
    "1. Let's say that things that 4, 5 stars are things that people liked. So those are positives. \n",
    "2. Things that 1 and 2 stars are negative. \n",
    "3. ignore all 3 star reviews.\n",
    "So I'm gonna say a positive sentiment equals 4 star or 5 star reviews. So let's go ahead and add a new column to our table that defines the actual sentiment. So products new column called sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will **ignore** all reviews with *rating = 3*, since they tend to have a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "285437"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reviews = reviews[reviews['review_overall'] != 3]\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will assign reviews with a rating of 4 or higher to be *positive* reviews, while the ones with rating of 2 or lower are *negative*. For the sentiment column, we use +1 for the positive class label and -1 for the negative class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------+------------+-------------------------------+\n",
      "| X1 | Unnamed: 0 |    asin    |            helpful            |\n",
      "+----+------------+------------+-------------------------------+\n",
      "| 0  |     0      | 097293751X | [None, 0, None, None, None... |\n",
      "| 1  |     1      | 097293751X | [None, 0, None, None, None... |\n",
      "+----+------------+------------+-------------------------------+\n",
      "+-------------------------------+-------------+----------------+\n",
      "|           reviewText          |  reviewTime |   reviewerID   |\n",
      "+-------------------------------+-------------+----------------+\n",
      "| Perfect for new parents. W... | 07 16, 2013 | A1HK2FQW6KXQB2 |\n",
      "| This book is such a life s... | 06 29, 2013 | A19K65VY14D13R |\n",
      "+-------------------------------+-------------+----------------+\n",
      "+-------------------------------+-------------------------------+----------------+-----+\n",
      "|          reviewerName         |            summary            | unixReviewTime | ... |\n",
      "+-------------------------------+-------------------------------+----------------+-----+\n",
      "| Amanda Johnsen \"Amanda E. ... |            Awesine            |   1373932800   | ... |\n",
      "|             angela            | Should be required for all... |   1372464000   | ... |\n",
      "+-------------------------------+-------------------------------+----------------+-----+\n",
      "[285437 rows x 13 columns]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "reviews['review_sentiment'] = reviews['review_overall'].apply(lambda rating : +1 if rating > 3 else -1)\n",
    "reviews.print_rows(num_rows=2, num_columns=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample dataset to make sure classes are balanced\n",
    "Just as we did in the previous assignment, we will undersample the larger class (safe loans) in order to balance out our dataset. This means we are throwing away many data points. We use `seed=1` so everyone gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of positive reviews             : 0.50116556825\n",
      "Percentage of negative reviews             : 0.49883443175\n",
      "Total number of reviews in our new dataset : 69494\n"
     ]
    }
   ],
   "source": [
    "positive_reviews_raw = reviews[reviews['review_sentiment'] == 1]\n",
    "negative_reviews_raw  = reviews[reviews['review_sentiment'] == -1]\n",
    "\n",
    "# Undersample the reviews.\n",
    "percentage = len(negative_reviews_raw)/float(len(positive_reviews_raw))\n",
    "negative_reviews = negative_reviews_raw\n",
    "positive_reviews = positive_reviews_raw.sample(percentage, seed=1)\n",
    "products = negative_reviews_raw.append(positive_reviews)\n",
    "\n",
    "print \"Percentage of positive reviews             :\", len(positive_reviews) / float(len(products))\n",
    "print \"Percentage of negative reviews             :\", len(negative_reviews) / float(len(products))\n",
    "print \"Total number of reviews in our new dataset :\", len(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 2/6\n",
    "Let us quickly explore more of this dataset.\n",
    "1. We count the number of positive and negative reviews.\n",
    "2. Modify the subset to contain similar numbers of positive and negative reviews, as the original dataset consisted primarily of positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of positive reviews = 250771\n",
      "# of negative reviews = 34666\n"
     ]
    }
   ],
   "source": [
    "print '# of positive reviews =', len(reviews[reviews['review_sentiment']==1])\n",
    "print '# of negative reviews =', len(reviews[reviews['review_sentiment']==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of positive balanced reviews = 34828\n",
      "# of negative balanced reviews = 34666\n"
     ]
    }
   ],
   "source": [
    "print '# of positive balanced reviews =', len(products[products['review_sentiment']==1])\n",
    "print '# of negative balanced reviews =', len(products[products['review_sentiment']==-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Extraction Phase: data preparation. \n",
    "## (desarrollado en el notebook Data Engineering)\n",
    "**Note:** \n",
    " - column review_clean with text cleaning developed in Data Engineering notebook\n",
    " - building features: la lista de palabras important_words.json se confecciona en el nb Data Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are several ways of doing this. We use the built-in *count* function for Python lists. Each **review without punctuation, stopwords, etc** string is first split into individual words and the number of occurances of a given word is counted.\n",
    "1. Transform the reviews into word-counts (only for **important_words**, without punctuation, stopwords, etc)\n",
    "2. For each word in **important_words**, we compute a count for the number of times the word occurs in the review. We will store this count in a separate column (one for each word). The result of this feature processing is a single column for each word in **important_words** which keeps a count of the number of times the respective word occurs in the review text.\n",
    "\n",
    "Now, we will load these words from this JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('important_words.json', 'r') as f: # Reads the list of words\n",
    "    important_words = json.load(f)\n",
    "important_words = [str(s) for s in important_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['wa', 'one', 'dog', 'like', 'love', 'baby', 'get', 'would', 'use', 'ha', 'great', 'cat', 'time', 'little', 'well', 'really', 'product', 'toy', 'also', 'work', 'good', 'dont', 'much', 'month', 'food', 'easy', 'make', 'put', 'old', 'even', 'seat', 'still', 'bought', 'keep', 'bag', 'used', 'thing', 'two', 'fit', 'back', 'go', 'bottle', 'first', 'doe', 'doesnt', 'around', 'small', 'day', 'im', 'got', 'using', 'size', 'need', 'take', 'water', 'think', 'son', 'year', 'since', 'better', '2', 'could', 'enough', 'didnt', 'recommend', 'way', 'buy', 'nice', 'made', 'diaper', 'lot', 'price', 'clean', 'treat', 'come', 'look', 'problem', 'long', 'big', 'bit', 'ive', 'see', 'daughter', 'car', 'hold', 'side', 'every', 'play', 'give', 'want', 'know', 'stroller', 'sure', 'right', '3', 'cant', 'week', 'perfect', 'seems', 'box', 'though', 'without', 'easily', 'tried', 'never', 'litter', 'cup', 'say', 'soft', 'new', 'pretty', 'another', 'cover', 'find', 'quality', 'best', 'hard', 'top', 'review', 'try', 'something', 'getting', 'large', 'last', 'part', 'plastic', 'help', 'room', 'however', 'going', 'far', 'color', 'different', 'happy', 'hand', 'able', 'many', 'found', 'child', 'u', 'always', '4', '5', 'purchased', 'feel', 'pad', 'set', 'smell', 'chew', 'pet', 'piece', 'actually', 'brand', 'light', 'away', 'issue', 'thought', 'le', 'item', 'place', 'definitely', 'ball', 'house', 'seem', 'wash', 'eat', 'night', 'worth', 'pump', 'bed', 'wish', 'star', 'puppy', 'store', 'minute', 'bottom', 'high', 'amazon', 'cute', 'sleep', 'dry', 'looking', 'started', 'crib', 'second', 'strap', 'isnt', 'purchase', 'took', 'quite', 'may', 'inside', 'anything', 'end', 'kid', 'almost', 'highly', 'open', 'money', 'chair', 'wont', 'came', 'fine', 'stay', 'bowl', 'comfortable', 'several', 'thats', 'three', '6', 'yet', 'easier', 'loved', 'extra', 'head', 'longer', 'probably', 'wanted', 'might', 'needed', 'change', 'smaller', 'couple', 'super', '1', 'sturdy', 'tank', 'home', 'floor', 'worked', 'sound', 'handle', 'trying', 'hour', 'let', 'nipple', 'everything', 'together', 'turn', 'buying', 'stuff', 'design', 'expensive', 'material', 'bad', 'ordered', 'pull', 'instead', 'wipe', 'filter', 'id', 'favorite', 'front', 'door', 'monitor', 'went', 'said', 'wouldnt', 'move', 'especially', 'theyre', 'maybe', 'wasnt', 'quickly', 'sometimes', 'blanket', 'regular', 'collar', 'pack', 'lb', 'kind', 'ever', 'free', '10', 'brush', 'either', 'shes', 'decided', 'next', 'teeth', 'larger', 'older', 'perfectly', 'people', 'gave', 'tray', 'plus', 'weight', 'stick', 'friend', 'least', 'run', 'havent', 'mouth', 'carrier', 'reason', 'toddler', 'start', 'pound', 'others', 'job', 'fabric', 'safe', 'must', 'read', 'leak', 'half', 'cloth', 'fun', 'milk', 'fall', 'sit', 'pillow', 'fact', 'walk', 'feature', 'mat', 'order', 'infant', 'battery', 'leg', 'vet', 'changing', 'feeding', 'pocket', 'whole', 'although', 'weve', 'fish', 'heavy', 'nothing', 'husband', 'hole', 'area', 'carry', 'youre', 'mix', 'already', 'hair', 'foot', 'etc', 'ingredient', 'difficult', 'kitty', 'table', 'outside', 'space', 'bigger', 'strong', 'lid', 'feed', 'mattress', 'liked', 'full', 'usually', 'tub', 'type', 'durable', 'case', '12', 'option', 'leave', 'couldnt', 'idea', 'close', 'gate', 'recommended', 'sheet', 'often', 'fold', 'overall', 'swing', 'system', 'cut', 'air', 'huge', 'absolutely', 'done', 'harness', 'cleaning', 'remove', 'amount', 'cheap', 'completely', 'rather', 'short', 'break', 'newborn', 'glad', 'life', 'animal', 'seemed', 'point', 'bath', 'noise', 'leash', 'ill', 'putting', 'wet', 'deal', 'simple', '8', 'making', 'warm', 'add', 'apart', 'boy', 'complaint', 'left', 'unit', 'throw', 'parent', 'potty', 'received', 'travel', 'tell', 'eating', 'picture', 'else', 'mind', 'playing', 'cost', 'girl', 'white', 'working', 'shape', 'mean', 'fast', 'snap', 'giving', 'training', 'within', 'care', 'velcro', 'chicken', 'four', 'cheaper', 'bib', 'wear', 'blue', 'mom', 'pacifier', 'gift', 'clip', 'mine', 'hot', 'medium', 'ago', 'music', 'arm', 'company', 'finally', 'inch', 'stop', 'natural', 'kitten', 'face', 'onto', 'chewing', 'anyone', 'odor', 'enjoy', 'looked', 'due', 'washing', 'kept', 'stand', 'crate', 'soon', 'drink', 'insert', '20', 'base', 'guess', 'real', 'along', 'thick', 'waste', 'save', 'roll', 'prefer', 'line', 'later', 'push', 'awesome', '15', 'low', 'ended', 'worry', 'arent', 'held', 'breast', 'pick', 'container', 'difference', 'skin', 'tiny', 'flavor', '7', 'ok', 'keeping', 'us', 'exactly', 'thin', 'seen', 'hang', 'bone', 'mess', 'instruction', 'wall', 'noticed', 'pup', 'level', 'excellent', 'button', 'version', 'healthy', 'wonderful', 'daily', 'clear', 'eye', 'tight', 'middle', 'flat', 'taking', 'trip', 'ring', 'position', 'black', 'holding', 'flea', 'attached', 'opening', 'sitting', 'control', 'reviewer', 'pain', 'solid', 'rest', 'given', 'support', 'sippy', 'neck', 'secure', '30', 'package', 'install', 'formula', 'safety', 'sleeping', 'green', 'storage', 'course', 'pleased', 'hope', 'immediately', 'yes', 'matter', 'spot', 'quick', 'replacement', 'edge', 'saw', 'cold', 'replace', 'nail', 'coat', 'live', 'nicely', 'believe', 'wheel', 'extremely', 'spoon', 'running', 'washed', 'liner', 'figure', 'bar', 'reach', 'fountain', 'rubber', 'taste', 'unless', 'crazy', 'setting', 'snack', 'expected', 'wide', 'meal', 'holder', 'cage', 'arrived', 'disappointed', 'hate', 'fill', '9', 'hit', 'cause', 'past', 'suction', 'gone', 'bright', 'stuck', 'spray', 'adjust', 'figured', 'lock', 'ear', 'fresh', 'brown', 'wrap', 'simply', 'show', 'flow', 'drop', 'standard', 'touch', 'unfortunately', 'hasnt', 'attach', 'model', 'otherwise', 'mirror', 'scoop', 'entire', 'dishwasher', 'lab', 'step', 'slightly', 'paw', 'return', 'straw', 'height', 'turned', 'shower', 'supposed', 'stopped', 'kong', 'guy', 'fairly', 'anyway', 'five', 'compared', 'purpose', 'double', 'chewer', 'finger', 'slide', 'watch', 'helped', 'teething', 'totally', 'twice', 'added', 'machine', 'useful', 'fur', 'amazing', 'similar', 'bird', 'latch', 'loud', 'mobile', 'someone', 'poop', 'born', 'age', 'wrong', 'per', 'gotten', 'kitchen', 'trouble', 'grab', 'pay', 'catch', 'summer', 'test', 'basket', 'tall', 'hear', 'constantly', 'excited', 'forward', 'handy', 'check', 'surface', 'medela', 'result', 'body', 'reading', 'red', 'picky', 'variety', 'lay', 'loose', 'sized', 'style', 'multiple', 'screw', 'shipping', 'important', 'family', 'corner', 'britax', 'single', 'convenient', 'local', 'window', 'plenty', 'coming', 'original', 'lasted', 'kibble', 'carseat', 'choice', 'metal', 'plan', 'gallon', 'spend', '100', 'felt', 'designed', 'interested', 'adult', 'camera', 'ready', 'allows', 'recently', 'video', 'zipper', 'tip', 'dirty', 'annoying', 'diet', 'toilet', 'avent', 'shoulder', 'value', 'near', 'hook', 'lost', 'normal', 'attention', 'carpet', 'refill', 'hoping', 'direction', 'dish', 'belt', 'everywhere', 'eats', 'wood', 'tear', 'continue', 'pee', 'expect', 'stain', 'cool', 'towel', 'switch', 'across', 'morning', 'lightweight', 'note', 'experience', 'nursing', 'asleep', 'pumping', 'solution', 'liquid', 'available', 'anymore', 'wait', 'based', 'hurt', 'squeaker', 'pop', 'sort', 'adjustable', 'knew', 'youll', 'bulky', 'nose', 'warmer', 'everyone', 'n', 'graco', 'thinking', 'aquarium', 'main', 'glass', 'pill', 'scratch', 'broke', 'clothes', 'purchasing', 'chewed', 'notice', 'eventually', 'pulling', 'bite', 'cotton', 'customer', 'worried', 'nap', 'walking', 'book', 'allow', 'length', 'interest', 'tube', 'enjoys', 'clipper', 'facing', 'number', 'okay', 'prevent', 'booster', 'nearly', 'properly', 'higher', 'helpful', 'except', 'supplement', 'moving', 'ground', 'owner', 'twin', 'sits', 'lower', 'temperature', 'power', 'foam', 'heat', 'behind', 'jump', 'surprised', 'become', 'sink', 'dr', 'pouch', 'cap', '25', 'catnip', 'service', 'spout', 'bathroom', 'quiet', 'ton', 'told', 'alone', 'mini', 'careful', 'texture', 'starting', 'young', 'included', 'yard', '14', 'barely', 'update', 'addition', 'pink', 'imagine', 'effective', 'wearing', 'winter', 'opened', 'mostly', 'furniture', 'carrying', 'bring', 'scratching', 'possible', 'oz', 'bassinet', 'anywhere', 'cry', 'certainly', 'adorable', 'swaddle', 'fantastic', 'grain', 'removed', 'breed', 'grow', 'spill', 'research', 'decent', 'kit', 'whatever', 'bother', 'device', 'true', 'hanging', 'returned', 'sun', 'unlike', 'flimsy', 'ride', 'human', 'pattern', 'negative', 'fan', 'online', 'spent', 'tree', 'tool', 'straight', 'opinion', 'frame', 'changed', 'living', 'learned', 'impressed', 'offer', 'covered', 'busy', 'placed', 'poor', 'pail', 'underneath', 'mentioned', 'shampoo', 'falling', 'mesh', 'softer', 'pulled', 'chance', 'taken', 'six', 'including', 'dried', 'organic', 'basically', 'broken', 'tummy', 'post', 'smooth', 'slip', 'plug', 'assemble', 'key', '18', 'health', 'thank', 'ordering', 'oil', 'moved', 'disposable', 'sent', 'sensitive', 'installed', 'shopping', 'urine', 'range', 'mouse', 'accident', 'buckle', 'tie', 'call', 'whether', 'center', 'fully', 'motor', 'special', 'market', 'person', 'hospital', 'harder', 'empty', 'impossible', 'cord', 'beautiful', 'meat', 'slow', 'packaging', 'dark', 'allergy', '50', 'likely', 'particular', 'fell', 'switched', 'previous', 'rating', 'sharp', 'list', 'realized', 'shell', 'tag', '11', 'vitamin', 'ate', 'rattle', 'tough', 'considering', 'match', 'comfy', 'name', 'today', 'closed', 'understand', 'manufacturer', 'suck', 'ease', 'phone', 'suggest', 'print', 'teether', 'grip', 'concerned', 'actual', 'provide', 'breath', 'perhaps', 'please', 'built', 'paid', 'toss', 'feeder', 'portion', 'yellow', 'despite', 'comfort', 'avoid', 'oh', 'rinse', 'separate', 'shake', 'none', 'active', 'shepherd', 'mixed', 'pricey', 'wife', 'rack', 'padding', 'deep', 'rear', 'consider', 'volume', 'trash', 'leaf', 'heard', 'waterproof', 'wake', 'future', 'called', 'tired', 'compartment', 'trick', 'alternative', '45', '13', 'dust', 'uncomfortable', 'third', 'terrier', 'cushion', 'tend', 'block', 'portable', 'nursery', 'drinking', 'necessary', 'leaving', 'cleaner', 'scent', 'useless', 'seal', 'rock', 'description', 'squeak', 'cleaned', 'beat', 'honestly', 'compact', 'function', 'remember', 'thanks', 'sold', 'ergo', 'chemical', 'effect', 'clump', 'canned', 'stuffed', 'flip', '40', 'lift', 'messy', 'stair', 'became', 'paper', 'kick', 'additional', 'ounce', 'plate', 'obviously', 'attachment', 'mention', 'replaced', 'period', 'filled', 'realize', 'leaking', 'stomach', 'process', 'hide', 'angle', 'drying', 'truly', 'literally', 'chest', 'originally', 'somewhat', 'stool', 'grass', 'chihuahua', 'feeling', 'hated', 'heater', 'mark', 'happen', 'con', 'supply', 'rough', 'strip', 'track', 'younger', 'pro', 'folded', 'absorbent', 'choose', 'dollar', 'whenever', 'afraid', 'thicker', 'normally', 'bunch', 'occasionally', 'early', 'cube', 'currently', 'usa', 'mother', 'protect', 'german', 'sack', 'china', 'watching', 'loop', 'calm', 'saved', 'content', 'major', 'lol', 'male', 'bounce', 'rip', 'comfortably', 'rescue', 'round', 'tail', 'sell', 'finding', 'prefers', 'initially', 'fancy', 'photo', 'grocery', 'lose', 'provides', 'tape', 'shade', 'spit', 'sling', 'chose', 'plant', 'valve', 'corn', 'laundry', 'priced', 'gentle', 'forever', 'comment', 'bouncer', 'gas', 'everyday', 'happened', 'pan', 'wonder', '16', 'various', 'bubble', 'stretch', 'freezer', 'dispenser', '23', 'concern', 'world', 'expecting', 'cart', 'washable', 'pressure', 'section', 'manual', 'flap', 'boppy', 'comb', 'bra', 'removable', 'pellet', 'positive', 'min', 'dryer', 'activity', 'snug', 'condition', 'fed', 'silicone', 'improvement', 'finish', 'terrible', 'eater', 'appears', 'canopy', 'destroy', 'played', 'stuffing', 'stage', 'picked', 'bc', 'protein', 'sweet', 'state', 'caught', 'awhile', 'learn', 'sick', 'traveling', '24', 'genie', 'shed', 'send', 'layer', 'degree', 'meant', 'enjoyed', 'website', 'city', 'plush', 'worse', 'ask', 'saying', 'access', 'wire', 'directly', 'squirrel', 'tug', 'breastfeeding', 'knowing', 'damage', 'brought', 'concept', 'fat', 'weather', 'gum', 'seeing', 'destroyed', 'learning', 'bjorn', 'firm', 'yr', 'sister', 'movement', 'inexpensive', 'squeeze', 'screen', 'entertained', 'certain', 'werent', 'total', 'spring', 'average', 'upright', 'couch', 'hopefully', 'string', 'drawer', 'biggest', 'frequently', 'rope', 'hell', 'satisfied', 'blade', 'fleece', 'counter', 'horrible', 'rail', 'elastic', 'downside', 'benefit', 'belly', 'tab', 'began', 'hip', 'depending', '35', 'chase', 'cabinet', 'effort', 'golden', 'agree', 'shelf', 'bedroom', 'charge', 'beef', 'rolling', 'attractive', 'recline', 'christmas', 'colorful', 'butter', 'sense', 'duck', 'happier', 'thus', 'zip', 'tends', 'turtle', 'fix', 'rate', 'peanut', 'lead', 'owned', 'freeze', 'click', 'shield', 'cardboard', 'word', 'seriously', 'beginning', 'vacuum', 'throwing', 'powder', 'bumper', 'adapter', 'growing', 'visit', 'sponge', 'contain', 'basic', 'burp', 'faster', 'joint', 'playtex', 'backpack', 'awkward', 'claim', 'nature', 'forget', 'bonus', 'x', 'bark', 'follow', 'transition', 'slept', 'laying', 'accessory', 'guard', 'luck', 'particularly', '34', 'trust', 'dental', 'label', 'dropped', 'apparently', 'correctly', 'dinner', 'padded', 'drive', '2nd', 'balance', 'relatively', 'ability', 'pregnant', 'fussy', 'rabbit', 'construction', 'cutting', 'threw', 'intended', 'press', 'ripped', 'rid', 'orange', 'reasonable', 'ice', 'term', 'park', 'bored', 'brushing', 'sale', 'weird', 'appreciate', 'mechanism', 'turning', 'chicco', 'train', 'adding', 'advertised', 'ran', 'proof', 'regularly', 'munchkin', 'miracle', 'complete', 'feather', 'discovered', 'upon', 'seam', 'stiff', 'salmon', 'temp', 'neither', 'mo', 'saver', 'fetch', 'fault', 'hassle', 'barking', 'risk', 'talk', 'pushing', 'shaped', 'flexible', 'buck', 'mommy', 'build', 'happens', 'climb', 'washer', 'daycare', 'seller', 'female', 'pair', 'breeze', 'securely', 'moment', 'waiting', 'adjustment', 'previously', 'clearly', 'lick', 'sign', 'traditional', 'sticking', 'minor', 'upset', 'rawhide', 'somewhere', 'protection', 'scratcher', 'correct', 'ideal', 'requires', 'bend', 'target', 'known', 'form', 'returning', 'managed', 'therefore', 'panel', 'require', 'generally', 'view', 'prime', 'cycle', 'speed', 'trim', 'personally', 'shut', 'stayed', 'affordable', 'spin', 'suppose', 'indoor', 'mode', 'doubt', 'raw', 'narrow', 'removing', 'steel', 'lack', 'soap', 'bug']\n"
     ]
    }
   ],
   "source": [
    "print important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 3/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SFrame **products** now contains one column for each of the **important_words**. As an example, the column **perfect** contains a count of the number of times the word **perfect** occurs in each of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dtype: int\n",
       "Rows: 69494\n",
       "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ... ]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['perfect']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write some code to compute the number of product reviews that contain the word **perfect**.\n",
    "* First create a column called `contains_perfect` which is set to 1 if the count of the word **perfect** (stored in column **perfect**) is >= 1.\n",
    "* Sum the number of 1s in the column `contains_perfect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products['contains_perfect'] = products['perfect'].apply(lambda s : +1 if s >= 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3516"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "products['contains_perfect'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing logistic regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## link function (estimating conditional probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lecture that the link function is given by:\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))},\n",
    "$$\n",
    "\n",
    "where the feature vector $h(\\mathbf{x}_i)$ represents the word counts of **important_words** in the review  $\\mathbf{x}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "produces probablistic estimate for P(y_i = +1 | x_i, w).\n",
    "estimate ranges between 0 and 1.\n",
    "'''\n",
    "\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    predictions = 1.0 / (1.0 + np.exp(-scores))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the link function works with matrix algebra?\n",
    "\n",
    "Since the word counts are stored as columns in **feature_matrix**, each $i$-th row of the matrix corresponds to the feature vector $h(\\mathbf{x}_i)$:\n",
    "$$\n",
    "[\\text{feature_matrix}] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T \\\\\n",
    "h(\\mathbf{x}_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "h_0(\\mathbf{x}_1) & h_1(\\mathbf{x}_1) & \\cdots & h_D(\\mathbf{x}_1) \\\\\n",
    "h_0(\\mathbf{x}_2) & h_1(\\mathbf{x}_2) & \\cdots & h_D(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_0(\\mathbf{x}_N) & h_1(\\mathbf{x}_N) & \\cdots & h_D(\\mathbf{x}_N)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "By the rules of matrix multiplication, the score vector containing elements $\\mathbf{w}^T h(\\mathbf{x}_i)$ is obtained by multiplying **feature_matrix** and the coefficient vector $\\mathbf{w}$.\n",
    "$$\n",
    "[\\text{score}] =\n",
    "[\\text{feature_matrix}]\\mathbf{w} =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T \\\\\n",
    "h(\\mathbf{x}_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\mathbf{w}\n",
    "= \\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T\\mathbf{w} \\\\\n",
    "h(\\mathbf{x}_2)^T\\mathbf{w} \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\\mathbf{w}\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\left[\n",
    "\\begin{array}{c}\n",
    "\\mathbf{w}^T h(\\mathbf{x}_1) \\\\\n",
    "\\mathbf{w}^T h(\\mathbf{x}_2) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{w}^T h(\\mathbf{x}_N)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute derivative of log likelihood with respect to a single coefficient\n",
    "\n",
    "Recall:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "Function that computes the derivative of log likelihood with respect to a single coefficient $w_j$. The function accepts two arguments:\n",
    "* `errors` vector containing $\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})$ for all $i$.\n",
    "* `feature` vector containing $h_j(\\mathbf{x}_i)$  for all $i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):     \n",
    "    # Compute the dot product of errors and feature\n",
    "    derivative = np.dot(errors, feature)\n",
    "    \n",
    "    # Return the derivative\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduced a transformation of this likelihood---called the log likelihood---that simplifies the derivation of the gradient and is more numerically stable.  Due to its numerical stability, we will use the log likelihood instead of the likelihood to assess the algorithm.\n",
    "\n",
    "The log likelihood is computed using the following formula (see the advanced optional video if you are curious about the derivation of this equation):\n",
    "\n",
    "$$\\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) $$\n",
    "\n",
    "Function to compute the log likelihood for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    logexp = np.log(1. + np.exp(-scores))\n",
    "    \n",
    "    # Simple check to prevent overflow\n",
    "    mask = np.isinf(logexp)\n",
    "    logexp[mask] = -scores[mask]\n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - logexp)\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking gradient steps\n",
    "Now we are ready to implement our own logistic regression. \n",
    "\n",
    "Function to solve the logistic regression model using gradient ascent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in xrange(max_iter):\n",
    "\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        for j in xrange(len(coefficients)): # loop over each coefficient\n",
    "            \n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            derivative = np.dot(errors,feature_matrix[:,j])\n",
    "            \n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            coefficients[j] = coefficients[j] + derivative*step_size\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, sentiment, coefficients)\n",
    "            print 'iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp)\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving a sentiment classifier with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets\n",
    "Let's perform a train/test split with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = products.random_split(.8, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of total reviews = 69494\n",
      "# of positive reviews on all data = 34828\n",
      "# of negative reviews on all data = 34666\n"
     ]
    }
   ],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "print '# of total reviews =', len(products)\n",
    "print '# of positive reviews on all data =', len(products[products['review_sentiment']==1])\n",
    "print '# of negative reviews on all data =', len(products[products['review_sentiment']==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of train_data reviews = 55656\n",
      "# of positive reviews on train data = 27945\n",
      "# of negative reviews on train data = 27711\n"
     ]
    }
   ],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "print '# of train_data reviews =', len(train_data)\n",
    "print '# of positive reviews on train data =', len(train_data[train_data['review_sentiment']==1])\n",
    "print '# of negative reviews on train data =', len(train_data[train_data['review_sentiment']==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# of test_data reviews = 13838\n",
      "# of positive reviews on test data = 6883\n",
      "# of negative reviews on test data = 6955\n"
     ]
    }
   ],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "print '# of test_data reviews =', len(test_data)\n",
    "print '# of positive reviews on test data =', len(test_data[test_data['review_sentiment']==1])\n",
    "print '# of negative reviews on test data =', len(test_data[test_data['review_sentiment']==-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFrame to NumPy array\n",
    "NumPy is a powerful library for doing matrix manipulation. Let us convert our data to matrices and then implement our algorithms with matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that extracts columns from an SFrame and converts them into a NumPy array. Two arrays are returned: one representing features and another representing class labels. The feature matrix includes an additional column 'intercept' to take account of the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_numpy_data(data_sframe, features, label):\n",
    "    data_sframe['intercept'] = 1\n",
    "    features = ['intercept'] + features\n",
    "    features_sframe = data_sframe[features]\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    label_sarray = data_sframe[label]\n",
    "    label_array = label_sarray.to_numpy()\n",
    "    return(feature_matrix, label_array)\n",
    "\n",
    "def get_numpy_feature_matrix(data_sframe, features):\n",
    "    data_sframe['intercept'] = 1\n",
    "    features = ['intercept'] + features\n",
    "    features_sframe = data_sframe[features]\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    return(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert the train_data into NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "train_feature_matrix, train_sentiment = get_numpy_data(train_data, important_words, 'review_sentiment') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(55656, 1501)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the sentiment classifier on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration   0: log likelihood of observed labels = -38567.67217537\n",
      "iteration   1: log likelihood of observed labels = -38557.57828161\n",
      "iteration   2: log likelihood of observed labels = -38547.51736595\n",
      "iteration   3: log likelihood of observed labels = -38537.48900294\n",
      "iteration   4: log likelihood of observed labels = -38527.49277553\n",
      "iteration   5: log likelihood of observed labels = -38517.52827490\n",
      "iteration   6: log likelihood of observed labels = -38507.59510030\n",
      "iteration   7: log likelihood of observed labels = -38497.69285890\n",
      "iteration   8: log likelihood of observed labels = -38487.82116559\n",
      "iteration   9: log likelihood of observed labels = -38477.97964291\n",
      "iteration  10: log likelihood of observed labels = -38468.16792082\n",
      "iteration  11: log likelihood of observed labels = -38458.38563660\n",
      "iteration  12: log likelihood of observed labels = -38448.63243468\n",
      "iteration  13: log likelihood of observed labels = -38438.90796651\n",
      "iteration  14: log likelihood of observed labels = -38429.21189044\n",
      "iteration  15: log likelihood of observed labels = -38419.54387155\n",
      "iteration  20: log likelihood of observed labels = -38371.61336775\n",
      "iteration  30: log likelihood of observed labels = -38277.68183865\n",
      "iteration  40: log likelihood of observed labels = -38186.10553173\n",
      "iteration  50: log likelihood of observed labels = -38096.66452711\n",
      "iteration  60: log likelihood of observed labels = -38009.17763362\n",
      "iteration  70: log likelihood of observed labels = -37923.49483082\n",
      "iteration  80: log likelihood of observed labels = -37839.49109338\n",
      "iteration  90: log likelihood of observed labels = -37757.06139585\n",
      "iteration 100: log likelihood of observed labels = -37676.11670578\n",
      "iteration 200: log likelihood of observed labels = -36935.39225911\n",
      "iteration 300: log likelihood of observed labels = -36294.63334671\n"
     ]
    }
   ],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "#Atenci√≥n el n√∫mero de ceros debe coincidir con el n√∫mero de palabras m√°s uno.\n",
    "sentiment_model_coefficients = logistic_regression(train_feature_matrix, train_sentiment, initial_coefficients=np.zeros(train_feature_matrix.shape[1]),\n",
    "                                   step_size=1e-7, max_iter=301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class predictions from scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class predictions for a data point $\\mathbf{x}$ can be computed from the coefficients $\\mathbf{w}$ using the following formula:\n",
    "$$\n",
    "\\hat{y}_i = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{x}_i^T\\mathbf{w} > 0 \\\\\n",
    "      -1 & \\mathbf{x}_i^T\\mathbf{w} \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Now, we will write some code to compute class predictions. We will do this in two steps:\n",
    "* **Step 1**: First compute the **scores** using **feature_matrix** and **coefficients** using a dot product.\n",
    "* **Step 2**: Using the formula above, compute the class predictions from the scores.\n",
    "\n",
    "Step 1 can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Compute the scores as a dot product between feature_matrix and coefficients.\n",
    "scores = np.dot(train_feature_matrix, sentiment_model_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: compute the class predictions using the **scores** obtained above:\n",
    "train_sentiment_predictions = map((lambda score: +1 if score > 0 else -1), scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy of the model\n",
    "\n",
    "We will now measure the classification accuracy of the model. \n",
    "In accuracy, instead of measuring the number of errors, we measure the number of correct classifications.\n",
    "So the ratio here is number of correct divided by total number of sentences. \n",
    "In terms of accuracy, the best possible value is 1, I've got all the sentences right. \n",
    "The classification accuracy can be computed as follows:\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified data points}}{\\mbox{# total data points}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "# Reviews   correctly classified = 41574\n",
      "# Reviews incorrectly classified = 14082\n",
      "# Reviews total                  = 55656\n",
      "-----------------------------------------------------\n",
      "Accuracy = 0.75\n"
     ]
    }
   ],
   "source": [
    "num_mistakes = (train_sentiment != train_sentiment_predictions).sum()\n",
    "accuracy = 1.0 * (len(train_data) - num_mistakes) / len(train_data)\n",
    "print \"-----------------------------------------------------\"\n",
    "print '# Reviews   correctly classified =', len(train_data) - num_mistakes\n",
    "print '# Reviews incorrectly classified =', num_mistakes\n",
    "print '# Reviews total                  =', len(train_data)\n",
    "print \"-----------------------------------------------------\"\n",
    "print 'Accuracy = %.2f' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 4/6\n",
    "## Which words contribute most to positive & negative sentiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to compute the \"**most positive words**\". These are words that correspond most strongly with positive reviews. In order to do this, we will first do the following:\n",
    "* Treat each coefficient as a tuple, i.e. (**word**, **coefficient_value**).\n",
    "* Sort all the (**word**, **coefficient_value**) tuples by **coefficient_value** in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_model_coefficients_without_intercept = list(sentiment_model_coefficients[1:]) # exclude intercept\n",
    "word_coefficient_tuples = [(word, coefficient) for word, coefficient in zip(important_words, sentiment_model_coefficients_without_intercept)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **word_coefficient_tuples** contains a sorted list of (**word**, **coefficient_value**) tuples. The first 10 elements in this list correspond to the words that are most positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twenty \"most positive\" words\n",
    "\n",
    "Now, we compute the 10 words that have the most positive coefficient values. These words are associated with positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('love', 0.11410190817677396),\n",
       " ('great', 0.078001569616378538),\n",
       " ('easy', 0.059474962231721981),\n",
       " ('little', 0.041006981029093073),\n",
       " ('well', 0.032159517162655263),\n",
       " ('ha', 0.028804532988303717),\n",
       " ('perfect', 0.027403115995678896),\n",
       " ('keep', 0.023784541933520444),\n",
       " ('nice', 0.021069908412643886),\n",
       " ('happy', 0.018661153671856813),\n",
       " ('use', 0.017115301139918264),\n",
       " ('best', 0.016505836581433422),\n",
       " ('highly', 0.016035339511669265),\n",
       " ('size', 0.015279077307122335),\n",
       " ('clean', 0.014685307493386646),\n",
       " ('bit', 0.014418119819198246),\n",
       " ('soft', 0.013705436307556079),\n",
       " ('stroller', 0.01340457920238829),\n",
       " ('price', 0.011997781186369886),\n",
       " ('car', 0.011873163917787475)]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_tuples[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Twenty \"most negative\" words\n",
    "\n",
    "Next, we repeat this exercise on the 10 most negative words.  That is, we compute the 10 words that have the most negative coefficient values. These words are associated with negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('wa', -0.090206278230560488),\n",
       " ('would', -0.054721781600754471),\n",
       " ('product', -0.034934557049225559),\n",
       " ('money', -0.034675517069877183),\n",
       " ('didnt', -0.033521302249218718),\n",
       " ('even', -0.030029249368523196),\n",
       " ('waste', -0.025808466995737498),\n",
       " ('thought', -0.023173521569509043),\n",
       " ('review', -0.021125065935373506),\n",
       " ('disappointed', -0.020129728210277423),\n",
       " ('back', -0.01956709449526112),\n",
       " ('dont', -0.019479644713542733),\n",
       " ('tried', -0.019210059491731143),\n",
       " ('return', -0.019019047422876304),\n",
       " ('work', -0.018452774282315662),\n",
       " ('maybe', -0.017676029601350882),\n",
       " ('better', -0.017211417788142929),\n",
       " ('plastic', -0.016995576475619164),\n",
       " ('could', -0.01669394358627093),\n",
       " ('doesnt', -0.015969749998866654)]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_coefficient_tuples[0:20]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set. Making predictions with logistic regression\n",
    "Now that a model is trained, we can make predictions on the **test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to convert test_data into the sparse matrix format first.\n",
    "# Warning: This may take a few minutes...\n",
    "test_feature_matrix, test_sentiment = get_numpy_data(test_data, important_words, 'review_sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Compute the scores as a dot product between feature_matrix and coefficients.\n",
    "scores = np.dot(test_feature_matrix, sentiment_model_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: compute the class predictions using the **scores** obtained above:\n",
    "test_sentiment_predictions = map((lambda score: +1 if score > 0 else -1), scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------\n",
      "# Reviews   correctly classified = 10324\n",
      "# Reviews incorrectly classified = 3514\n",
      "# Reviews total                  = 13838\n",
      "-----------------------------------------------------\n",
      "Accuracy = 0.75\n"
     ]
    }
   ],
   "source": [
    "num_test_mistakes = (test_sentiment != test_sentiment_predictions).sum()\n",
    "accuracy = 1.0 * (len(test_data) - num_test_mistakes) / len(test_data)\n",
    "print \"-----------------------------------------------------\"\n",
    "print '# Reviews   correctly classified =', len(test_data) - num_test_mistakes\n",
    "print '# Reviews incorrectly classified =', num_test_mistakes\n",
    "print '# Reviews total                  =', len(test_data)\n",
    "print \"-----------------------------------------------------\"\n",
    "print 'Accuracy = %.2f' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 5/6\n",
    "## Applying the learned model to understand sentiment for reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['predicted_sentiment'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_data.sort('predicted_sentiment', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">predicted_sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review_sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review_overall</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">reviewText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.27683584121</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I am  dog trainer, and<br>bought this 3 years ago. ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.971094563624</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I love this car seat and<br>so do my kids.  My ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.921245515177</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I got these from toys r<br>us which is slightly ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.917245399385</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">2.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">As parents of two little<br>ones, I'd like to say we ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0.864708573875</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">5.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">This is the best purchase<br>we have made for our  ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[5 rows x 4 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tpredicted_sentiment\tfloat\n",
       "\treview_sentiment\tint\n",
       "\treview_overall\tfloat\n",
       "\treviewText\tstr\n",
       "\n",
       "Rows: 5\n",
       "\n",
       "Data:\n",
       "+---------------------+------------------+----------------+\n",
       "| predicted_sentiment | review_sentiment | review_overall |\n",
       "+---------------------+------------------+----------------+\n",
       "|    1.27683584121    |        1         |      5.0       |\n",
       "|    0.971094563624   |        1         |      5.0       |\n",
       "|    0.921245515177   |        1         |      5.0       |\n",
       "|    0.917245399385   |        -1        |      2.0       |\n",
       "|    0.864708573875   |        1         |      5.0       |\n",
       "+---------------------+------------------+----------------+\n",
       "+-------------------------------+\n",
       "|           reviewText          |\n",
       "+-------------------------------+\n",
       "| I am  dog trainer, and bou... |\n",
       "| I love this car seat and s... |\n",
       "| I got these from toys r us... |\n",
       "| As parents of two little o... |\n",
       "| This is the best purchase ... |\n",
       "+-------------------------------+\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[['predicted_sentiment','review_sentiment','review_overall','reviewText']][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I am  dog trainer, and bought this 3 years ago. My dogs love it, shelties, and my clients love it. All my board and train dogs love it. Easy set up, easy fold, easy wash cover.  I do not put the cover in the dyer! just air dry. this is sheltie, 23-30 lb weight. go bigger for size. has held up great!  truly a great buy ! for a non chewing dog.  dog lady of forest grove*** my clients have been buying this cot and all say they love it,, dogs really like the feel and support12 clients dog have loved it and demanded their own, a great buy.nov 2011 six more client dogs demand this bed,, great bed buy it.2012 dog bed has needed minor sewing repairs after 80 plus lb dogs want this bed. clients dogs love it. a great buy.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most positive review\n",
    "test_data[0]['reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_data.sort('predicted_sentiment', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">predicted_sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review_sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">review_overall</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">reviewText</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-2.35811830783</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">So I read many reviews on<br>this product and watched ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-2.11452775972</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">My advice on the Bright<br>Starts Sugar Blossom ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1.87059076415</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">4.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">First, I got this bark<br>collar after I read ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1.5866223393</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I got this monitor<br>because our old one was ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1.56127603302</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-1</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">1.0</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">I purchased the monitor<br>through Amazon and was ...</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[5 rows x 4 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tpredicted_sentiment\tfloat\n",
       "\treview_sentiment\tint\n",
       "\treview_overall\tfloat\n",
       "\treviewText\tstr\n",
       "\n",
       "Rows: 5\n",
       "\n",
       "Data:\n",
       "+---------------------+------------------+----------------+\n",
       "| predicted_sentiment | review_sentiment | review_overall |\n",
       "+---------------------+------------------+----------------+\n",
       "|    -2.35811830783   |        -1        |      1.0       |\n",
       "|    -2.11452775972   |        -1        |      1.0       |\n",
       "|    -1.87059076415   |        1         |      4.0       |\n",
       "|    -1.5866223393    |        -1        |      1.0       |\n",
       "|    -1.56127603302   |        -1        |      1.0       |\n",
       "+---------------------+------------------+----------------+\n",
       "+-------------------------------+\n",
       "|           reviewText          |\n",
       "+-------------------------------+\n",
       "| So I read many reviews on ... |\n",
       "| My advice on the Bright St... |\n",
       "| First, I got this bark col... |\n",
       "| I got this monitor because... |\n",
       "| I purchased the monitor th... |\n",
       "+-------------------------------+\n",
       "[5 rows x 4 columns]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data[['predicted_sentiment','review_sentiment','review_overall','reviewText']][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"So I read many reviews on this product and watched youtube videos on it as well. Everyone said the same thing and shame on me for not following my gut feeling. I've been in the aquarium hobby for years and this was my first attempt at setting this up the way its supposed to be done according to the directions. First of all, everyone said it leaks. Well, it does, and from a few places. The hose input and output leaks, the bottom screw on access point leaks and overall this is a cheaply made crappy product. Now, am I saying it doesn't work? IDK, but its not working for me simply on the fact that I shouldn't have to completely seal and bond every point of this contraption and use hose clamps which still leak. The directions may as well be a pop up book with no words, unless  you can read some vague chineese symbols if the directions translate into English. It never says what appropriate size hose to use, I tried 1/2 inch and it fits fine, but still leaks. 3/8 seemed too tight. 5/8 too big. The suction cups that are supposed to hold the unit mounted to the tank aren't even designed for this unit. The suction cup plastic nipple molded into the unit that is supposed to go into the suction cup hole, for one isn't big enough, for two even if you did get the nipple in without breaking it off it won't be held by anything inside of it because there is no inner lip that catches the fatter part of the nipple to hold the suction cup in. Instead it looks like there needs to be a larger bracket or slot that the suction cup tip slides into a hole and then holds the unilt to the suction cup, but there is surely no bracket for that whatsoever so that deems the suctions cups useless. Just a P*SS POOR design on that end. As for the overall functionality, from a design aspect the concept seems cool and would probably work as others have mentioned, but what does your time cost you? I spend about 2 hours trying to get this thing hooked up and running which took about 30 minutes at most between the hoses and my filter etc, but then the next 1.5 hours it took me to &#34;ATTEMPT&#34; to get this thing to stop leaking. I tried teflon tape, no go. Then added a bit of plumbers dope, no go. Cleaned that off and aside from probably needing to silicone the hole thing shut for good, I stopped there. Then came the hose ends leaking. Every attempt to see how the  water was leaking and to get it to stop was failed. And trust me I've remodeled houses and I do everything from plumbing to electrical and this thing doesn't serve its purpose. This needs to have barbed tips to hold the hose tighter than not and should come with some sort of hose clamps or even really secure zip locks if and when the correct hose that &#34;they suggest&#34; is used properly, but oh yeah they didn't do that. I don't feel I should have to set up metal hose clamps on a product like this. Oh, I forgot to mention for the little while that this was running the impellers would only spin when the unit was in a horizontal position. The directions say to keep it in an upright position at all times, YEAH RIGHT!! Everytime you put it in an upright position the impellers stop spinning. On one of my leak fixing attempts I took the main inner shaft out and it cracked too. Its like having an eggshell inner tube, instead of a nice rigid plastic durable inner tube that should be able to last. The plastic impeller blades are crap too. Overall this was a huge waste of time and money and I don't care how many people had to &#34;JIMMY RIG&#34; this thing to get it to function properly, but right out of the box this thing was a lost cause. If you are that serious about diffusing CO2 then I suggest taking more time to figure out another way instead of this as you may spend less time fussing with it, or be a bit creative and find a way to mimic this simple design if it works so well and build your own unit. I'm pretty much done with this plastic hunk of cheaply made chineese junk. I'm sure if an american company were making this it would have some sort of better standard, at least I would hope so. I'm not going to bother with customer service or returning it etc because at this point what are they going to say? Well, you modified it so it voided the warranty! Well, duh, no matter who you are or what you are using this for you are going to have to modify it and tinker with its major major flaws of design, material, and functionality so basically your warranty is already void before you purchase this. I usually give every product a fair shake, but this one by far is a total bummer. I only gave it 1 star because it arrived and was in a new box. Whoo hoo!! That's about all you will cheer for with this one. Like I said, I'm either going to have to make my own unit or pretty much settle on a ceramic diffuser that only costs  a few bucks compared to this crap. I wouldn't have paid 3 dollars for this thing at a flea market if I knew this would be the case. I think Amazon needs to take this one off the website as it requires major mods and a waste of time. New products that boast what this one boasts should not need to be modified or re-engineered in any way or form and if consistent leaks were the issue it should be a simple call to replace a faulty unit as it was made faulty. Seriously, if this review hasn't changed your mind then be my guest and give yourself a headache and thin out your wallet a little because that's about all I can say you will accomplish with this one. I'll make my own!! Complete waste.....and probably an outstanding concept and design in what it's supposed to do &#34;if it worked&#34; and was made with better material and simple design details were some sort of concern to the people making this. This looks like some kid's science fair project that he was making out of his garage in plastic molds with zero quality control and product testing. but aside from that this thing could have come in a cracker jack box and been worth more. JUNK!! Amazon, stop selling this product. I love you guys, but seriously this one is a joke overall. I don't want to be vulgar, but I've had &#34;protective latex gear&#34;, if you will, last longer than this.\""
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Most negative review\n",
    "test_data[0]['reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 6/6: \n",
    "# TODO: Applying the learned model to discover insights on twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Extraction Phase: data preparation. \n",
    "### (desarrollado en el notebook Data Engineering)\n",
    "**Note:** \n",
    " - column text_clean with text cleaning developed in Data Engineering notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"4114\",\"737948289509654529\",\"6:04am: sunrise\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"4114\",\"737948289509654529\",\"6:04am: sunrise\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"2794\",\"737941761129992192\",\"@ebbtideapp Tide in Point Au Fer, Louisiana 06/01/2016\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"2794\",\"737941761129992192\",\"@ebbtideapp Tide in Point Au Fer, Louisiana 06/01/2016\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"1440\",\"737930256863113216\",\"hey vsauce michael here\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"1440\",\"737930256863113216\",\"hey vsauce michael here\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"11\",\"737913129292730368\",\"holy shit\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"11\",\"737913129292730368\",\"holy shit\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"sunset will be at 8:45pm\",\"Wed Jun 01 10:05:57 +0000 2016\",\"[38.35, -81.63]\",\"Charleston, WV\",\"604am sunrise sunset 845pm\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"sunset will be at 8:45pm\",\"Wed Jun 01 10:05:57 +0000 2016\",\"[38.35, -81.63]\",\"Charleston, WV\",\"604am sunrise sunset 845pm\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \" Low  5:14am  1.2\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \" Low  5:14am  1.2\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"do y'all got games on your phone\",\"Wed Jun 01 08:54:17 +0000 2016\",\"[40.86970401, -73.89353064]\",\"Bronx, NY\",\"hey vsauce michael yall got game phone\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"do y'all got games on your phone\",\"Wed Jun 01 08:54:17 +0000 2016\",\"[40.86970401, -73.89353064]\",\"Bronx, NY\",\"hey vsauce michael yall got game phone\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"@thebottlemen @ Webster Hall https://t.co/9Gh0dnaEih\",\"Wed Jun 01 07:46:14 +0000 2016\",\"[40.7319055, -73.9896578]\",\"Manhattan, NY\",\"holy shit thebottlemen Webster Hall http tco9Gh0dnaEih\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"@thebottlemen @ Webster Hall https://t.co/9Gh0dnaEih\",\"Wed Jun 01 07:46:14 +0000 2016\",\"[40.7319055, -73.9896578]\",\"Manhattan, NY\",\"holy shit thebottlemen Webster Hall http tco9Gh0dnaEih\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"5518\",\"737958911735529472\",\"#Top3Apps for 'Clinton 45'\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"5518\",\"737958911735529472\",\"#Top3Apps for 'Clinton 45'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"High 12:48pm  1.8\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"High 12:48pm  1.8\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"Twitter for iPhone 38%\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"Twitter for iPhone 38%\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>924 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "924 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/digeno/Documents/Cursos/BigData/Kschool-Data-Scientist/proyecto/git/us_tweets.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/digeno/Documents/Cursos/BigData/Kschool-Data-Scientist/proyecto/git/us_tweets.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 100 lines in 0.056327 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 100 lines in 0.056327 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"11\",\"737913129292730368\",\"holy shit\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"11\",\"737913129292730368\",\"holy shit\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"4114\",\"737948289509654529\",\"6:04am: sunrise\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"4114\",\"737948289509654529\",\"6:04am: sunrise\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"2794\",\"737941761129992192\",\"@ebbtideapp Tide in Point Au Fer, Louisiana 06/01/2016\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"2794\",\"737941761129992192\",\"@ebbtideapp Tide in Point Au Fer, Louisiana 06/01/2016\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"1440\",\"737930256863113216\",\"hey vsauce michael here\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"1440\",\"737930256863113216\",\"hey vsauce michael here\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"@thebottlemen @ Webster Hall https://t.co/9Gh0dnaEih\",\"Wed Jun 01 07:46:14 +0000 2016\",\"[40.7319055, -73.9896578]\",\"Manhattan, NY\",\"holy shit thebottlemen Webster Hall http tco9Gh0dnaEih\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"@thebottlemen @ Webster Hall https://t.co/9Gh0dnaEih\",\"Wed Jun 01 07:46:14 +0000 2016\",\"[40.7319055, -73.9896578]\",\"Manhattan, NY\",\"holy shit thebottlemen Webster Hall http tco9Gh0dnaEih\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"sunset will be at 8:45pm\",\"Wed Jun 01 10:05:57 +0000 2016\",\"[38.35, -81.63]\",\"Charleston, WV\",\"604am sunrise sunset 845pm\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"sunset will be at 8:45pm\",\"Wed Jun 01 10:05:57 +0000 2016\",\"[38.35, -81.63]\",\"Charleston, WV\",\"604am sunrise sunset 845pm\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \" Low  5:14am  1.2\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \" Low  5:14am  1.2\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"do y'all got games on your phone\",\"Wed Jun 01 08:54:17 +0000 2016\",\"[40.86970401, -73.89353064]\",\"Bronx, NY\",\"hey vsauce michael yall got game phone\"\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"do y'all got games on your phone\",\"Wed Jun 01 08:54:17 +0000 2016\",\"[40.86970401, -73.89353064]\",\"Bronx, NY\",\"hey vsauce michael yall got game phone\"\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"5518\",\"737958911735529472\",\"#Top3Apps for 'Clinton 45'\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"5518\",\"737958911735529472\",\"#Top3Apps for 'Clinton 45'\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"\"6690\",\"737995754187423744\",\"#GlobalRunningDay #TheWalkingWalt \"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"\"6690\",\"737995754187423744\",\"#GlobalRunningDay #TheWalkingWalt \""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"High 12:48pm  1.8\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"High 12:48pm  1.8\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Unable to parse line \"Twitter for iPhone 38%\"</pre>"
      ],
      "text/plain": [
       "Unable to parse line \"Twitter for iPhone 38%\""
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>930 lines failed to parse correctly</pre>"
      ],
      "text/plain": [
       "930 lines failed to parse correctly"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Finished parsing file /Users/digeno/Documents/Cursos/BigData/Kschool-Data-Scientist/proyecto/git/us_tweets.csv</pre>"
      ],
      "text/plain": [
       "Finished parsing file /Users/digeno/Documents/Cursos/BigData/Kschool-Data-Scientist/proyecto/git/us_tweets.csv"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre>Parsing completed. Parsed 5623 lines in 0.046208 secs.</pre>"
      ],
      "text/plain": [
       "Parsing completed. Parsed 5623 lines in 0.046208 secs."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------------------\n",
      "Inferred types from first 100 line(s) of file as \n",
      "column_type_hints=[int,int,str,str,array,str,str]\n",
      "If parsing fails due to incorrect types, you can correct\n",
      "the inferred type list above and pass it to read_csv in\n",
      "the column_type_hints argument\n",
      "------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "tweets_data = sframe.SFrame('us_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method SFrame.column_names of Columns:\n",
       "\tX1\tint\n",
       "\tid\tint\n",
       "\ttext\tstr\n",
       "\tcreated_at\tstr\n",
       "\tgeo\tarray\n",
       "\tcity\tstr\n",
       "\ttext_clean\tstr\n",
       "\n",
       "Rows: 5623\n",
       "\n",
       "Data:\n",
       "+----+--------------------+-------------------------------+\n",
       "| X1 |         id         |              text             |\n",
       "+----+--------------------+-------------------------------+\n",
       "| 0  | 737913052050423808 | Supplemental Health Care #... |\n",
       "| 1  | 737913053119959040 | Can you recommend anyone f... |\n",
       "| 2  | 737913055338713089 |    https://t.co/DZVgUmv8ZQ    |\n",
       "| 6  | 737913086837940226 | HCR108 [Passed] Requests t... |\n",
       "| 7  | 737913096543588352 | SB64 [Vetoed] Baltimore Co... |\n",
       "| 8  | 737913098217132032 | Just posted a video @ Rol-... |\n",
       "| 9  | 737913106752540672 | HB349 [Passed] Baltimore C... |\n",
       "| 12 | 737913131683446784 | Website Hacks to Trap more... |\n",
       "| 13 | 737913141993078784 | I'm at C-View Inn in Cape ... |\n",
       "| 14 | 737913142232162304 | geo e92dc4e56c6b97210b3383... |\n",
       "+----+--------------------+-------------------------------+\n",
       "+--------------------------------+------------------------------+\n",
       "|           created_at           |             geo              |\n",
       "+--------------------------------+------------------------------+\n",
       "| Wed Jun 01 07:45:56 +0000 2016 |  [37.8646149, -122.2341905]  |\n",
       "| Wed Jun 01 07:45:56 +0000 2016 |  [34.0522342, -118.2436849]  |\n",
       "| Wed Jun 01 07:45:56 +0000 2016 | [37.69944509, -123.01210475] |\n",
       "| Wed Jun 01 07:46:04 +0000 2016 |   [30.456615, -91.187356]    |\n",
       "| Wed Jun 01 07:46:06 +0000 2016 |   [38.978862, -76.490685]    |\n",
       "| Wed Jun 01 07:46:07 +0000 2016 |    [25.88414, -80.24274]     |\n",
       "| Wed Jun 01 07:46:09 +0000 2016 |   [38.978862, -76.490685]    |\n",
       "| Wed Jun 01 07:46:15 +0000 2016 | [47.07947453, -120.00366211] |\n",
       "| Wed Jun 01 07:46:17 +0000 2016 | [38.94637803, -74.91018722]  |\n",
       "| Wed Jun 01 07:46:17 +0000 2016 |   [37.781157, -122.39872]    |\n",
       "+--------------------------------+------------------------------+\n",
       "+-------------------+-------------------------------+\n",
       "|        city       |           text_clean          |\n",
       "+-------------------+-------------------------------+\n",
       "|    Oakland, CA    | Supplemental Health Care H... |\n",
       "|  Los Angeles, CA  | Can recommend anyone Nursi... |\n",
       "|  California, USA  |       http tcoDZVgUmv8ZQ      |\n",
       "|  Baton Rouge, LA  | HCR108 Passed Requests Dep... |\n",
       "|   Annapolis, MD   | SB64 Vetoed Baltimore Coun... |\n",
       "|    Westview, FL   | Just posted video RolLexx ... |\n",
       "|   Annapolis, MD   | HB349 Passed Baltimore Cou... |\n",
       "|  Washington, USA  | Website Hacks Trap Leads I... |\n",
       "|    Cape May, NJ   | I m CView Inn Cape May NJ ... |\n",
       "| San Francisco, CA | geo e92dc4e56c6b97210b3383... |\n",
       "+-------------------+-------------------------------+\n",
       "[5623 rows x 7 columns]\n",
       "Note: Only the head of the SFrame is printed.\n",
       "You can use print_rows(num_rows=m, num_columns=n) to print more rows and columns.>"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data.column_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    tweets_data[word] = tweets_data['text_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to convert test_data into the sparse matrix format first.\n",
    "# Warning: This may take a few minutes...\n",
    "tweets_feature_matrix  = get_numpy_feature_matrix(tweets_data, important_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Compute the scores as a dot product between feature_matrix and coefficients.\n",
    "scores = np.dot(tweets_feature_matrix, sentiment_model_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_data['predicted_sentiment'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to json array to d3 visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_data = tweets_data.sort('predicted_sentiment', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">predicted_sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">text</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">geo</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">city</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.22327507797</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">@BoxingIsBack<br>@NextBigFightPod i know ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[40.7122535, -73.6609331]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.105390381736</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Saw the DC flag and<br>thought it was the ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[38.943869, -76.736521]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.103835045922</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Rats! Too slow. I was<br>customer number 2. If ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[42.081468, -87.937746]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0971013626118</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">North Korea made a<br>Facebook clone and it ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[26.350777, -80.069251]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.095808594253</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">All I wanted was the<br>#BigApple and we got it! ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[40.75683419,<br>-73.84602157] ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0894200312681</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Did you know that<br>#BadSummerCampNames was ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[36.1719, -115.14]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0884172595864</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Fly were. All can't them<br>all he stars bring after ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[37.69911162,<br>-123.01205637] ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0881816507791</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Did you know that<br>#TheBachelorette was ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[45.5118, -122.6756]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0868389583698</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Did you know that<br>#MemorialDay2016 was ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[39.74, -104.9923]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0865893837259</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Did you know that<br>#codecon was Trending ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[37.7792, -122.4201]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 4 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tpredicted_sentiment\tfloat\n",
       "\ttext\tstr\n",
       "\tgeo\tarray\n",
       "\tcity\tint\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+---------------------+-------------------------------+\n",
       "| predicted_sentiment |              text             |\n",
       "+---------------------+-------------------------------+\n",
       "|    -0.22327507797   | @BoxingIsBack @NextBigFigh... |\n",
       "|   -0.105390381736   | Saw the DC flag and though... |\n",
       "|   -0.103835045922   | Rats! Too slow. I was cust... |\n",
       "|   -0.0971013626118  | North Korea made a Faceboo... |\n",
       "|   -0.095808594253   | All I wanted was the #BigA... |\n",
       "|   -0.0894200312681  | Did you know that #BadSumm... |\n",
       "|   -0.0884172595864  | Fly were. All can't them a... |\n",
       "|   -0.0881816507791  | Did you know that #TheBach... |\n",
       "|   -0.0868389583698  | Did you know that #Memoria... |\n",
       "|   -0.0865893837259  | Did you know that #codecon... |\n",
       "+---------------------+-------------------------------+\n",
       "+------------------------------+------+\n",
       "|             geo              | city |\n",
       "+------------------------------+------+\n",
       "|  [40.7122535, -73.6609331]   |  0   |\n",
       "|   [38.943869, -76.736521]    |  0   |\n",
       "|   [42.081468, -87.937746]    |  0   |\n",
       "|   [26.350777, -80.069251]    |  0   |\n",
       "| [40.75683419, -73.84602157]  |  0   |\n",
       "|      [36.1719, -115.14]      |  0   |\n",
       "| [37.69911162, -123.01205637] |  0   |\n",
       "|     [45.5118, -122.6756]     |  0   |\n",
       "|      [39.74, -104.9923]      |  0   |\n",
       "|     [37.7792, -122.4201]     |  0   |\n",
       "+------------------------------+------+\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data[['predicted_sentiment','text','geo','city']][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_tweets_data = tweets_data[['predicted_sentiment','text','geo','city']][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_tweets_data.export_json('d3/data/USA-positive.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Most negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tweets_data = tweets_data.sort('predicted_sentiment', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\"><table frame=\"box\" rules=\"cols\">\n",
       "    <tr>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">predicted_sentiment</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">text</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">geo</th>\n",
       "        <th style=\"padding-left: 1em; padding-right: 1em; text-align: center\">city</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.22327507797</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">@BoxingIsBack<br>@NextBigFightPod i know ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[40.7122535, -73.6609331]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.105390381736</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Saw the DC flag and<br>thought it was the ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[38.943869, -76.736521]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.103835045922</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Rats! Too slow. I was<br>customer number 2. If ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[42.081468, -87.937746]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0971013626118</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">North Korea made a<br>Facebook clone and it ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[26.350777, -80.069251]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.095808594253</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">All I wanted was the<br>#BigApple and we got it! ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[40.75683419,<br>-73.84602157] ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0894200312681</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Did you know that<br>#BadSummerCampNames was ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[36.1719, -115.14]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0884172595864</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Fly were. All can't them<br>all he stars bring after ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[37.69911162,<br>-123.01205637] ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0881816507791</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Did you know that<br>#TheBachelorette was ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[45.5118, -122.6756]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0868389583698</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Did you know that<br>#MemorialDay2016 was ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[39.74, -104.9923]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">-0.0865893837259</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">Did you know that<br>#codecon was Trending ...</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">[37.7792, -122.4201]</td>\n",
       "        <td style=\"padding-left: 1em; padding-right: 1em; text-align: center; vertical-align: top\">0</td>\n",
       "    </tr>\n",
       "</table>\n",
       "[10 rows x 4 columns]<br/>\n",
       "</div>"
      ],
      "text/plain": [
       "Columns:\n",
       "\tpredicted_sentiment\tfloat\n",
       "\ttext\tstr\n",
       "\tgeo\tarray\n",
       "\tcity\tint\n",
       "\n",
       "Rows: 10\n",
       "\n",
       "Data:\n",
       "+---------------------+-------------------------------+\n",
       "| predicted_sentiment |              text             |\n",
       "+---------------------+-------------------------------+\n",
       "|    -0.22327507797   | @BoxingIsBack @NextBigFigh... |\n",
       "|   -0.105390381736   | Saw the DC flag and though... |\n",
       "|   -0.103835045922   | Rats! Too slow. I was cust... |\n",
       "|   -0.0971013626118  | North Korea made a Faceboo... |\n",
       "|   -0.095808594253   | All I wanted was the #BigA... |\n",
       "|   -0.0894200312681  | Did you know that #BadSumm... |\n",
       "|   -0.0884172595864  | Fly were. All can't them a... |\n",
       "|   -0.0881816507791  | Did you know that #TheBach... |\n",
       "|   -0.0868389583698  | Did you know that #Memoria... |\n",
       "|   -0.0865893837259  | Did you know that #codecon... |\n",
       "+---------------------+-------------------------------+\n",
       "+------------------------------+------+\n",
       "|             geo              | city |\n",
       "+------------------------------+------+\n",
       "|  [40.7122535, -73.6609331]   |  0   |\n",
       "|   [38.943869, -76.736521]    |  0   |\n",
       "|   [42.081468, -87.937746]    |  0   |\n",
       "|   [26.350777, -80.069251]    |  0   |\n",
       "| [40.75683419, -73.84602157]  |  0   |\n",
       "|      [36.1719, -115.14]      |  0   |\n",
       "| [37.69911162, -123.01205637] |  0   |\n",
       "|     [45.5118, -122.6756]     |  0   |\n",
       "|      [39.74, -104.9923]      |  0   |\n",
       "|     [37.7792, -122.4201]     |  0   |\n",
       "+------------------------------+------+\n",
       "[10 rows x 4 columns]"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tweets_data[['predicted_sentiment','text','geo','city']][0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_tweets_data = tweets_data[['predicted_sentiment','text','geo','city']][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_tweets_data.export_json('d3/data/USA-negative.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most positives and negatives tweets of EEUU dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "def serve_html():\n",
    "    fn= './d3/html/d3_6_US_map_tooltips.html'\n",
    "    return IFrame(fn, 985, 570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "serve_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
