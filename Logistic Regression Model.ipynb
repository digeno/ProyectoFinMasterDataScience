{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Logistic Regression Model: Predicting sentiment from product reviews\n",
    "\n",
    "\n",
    "The goal of this first notebook is to explore logistic regression and feature engineering with a model created from scratch.\n",
    "\n",
    "In this notebook we will use product review data from Amazon.com to predict whether the sentiments about a product (from its reviews) are positive or negative.\n",
    "## Fire up [Sframe](https://github.com/dato-code/SFrame)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data\n",
    "amazon_reviews.csv es la unión de varios datasets (ver notebook Proyecto_DSC_DIRJAA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "reviews = sframe.SFrame('amazon_reviews.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 1/6\n",
    "Let us quickly explore more of this dataset.\n",
    "1. We count the number of positive and negative reviews \n",
    "2. list the first 10 products in the dataset.\n",
    "3. **TODO** Histogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "len(reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews.print_rows(num_rows=2, num_columns=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Engineering: defining reviews with positive or negative sentiment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll call data engineering, just defining what is a positive and negative sentiment. So let's do that right now. So in the subsection we're gonna define what's a positive and a negative sentiment.\n",
    "And so I'm gonna make an arbitrary choice here:\n",
    "1. Let's say that things that 4, 5 stars are things that people liked. So those are positives. \n",
    "2. Things that 1 and 2 stars are negative. \n",
    "3. ignore all 3 star reviews.\n",
    "So I'm gonna say a positive sentiment equals 4 star or 5 star reviews. So let's go ahead and add a new column to our table that defines the actual sentiment. So products new column called sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will **ignore** all reviews with *rating = 3*, since they tend to have a neutral sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews = reviews[reviews['overall'] != 3]\n",
    "len(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will assign reviews with a rating of 4 or higher to be *positive* reviews, while the ones with rating of 2 or lower are *negative*. For the sentiment column, we use +1 for the positive class label and -1 for the negative class label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "reviews['sentiment'] = reviews['overall'].apply(lambda rating : +1 if rating > 3 else -1)\n",
    "reviews.print_rows(num_rows=2, num_columns=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample dataset to make sure classes are balanced\n",
    "Just as we did in the previous assignment, we will undersample the larger class (safe loans) in order to balance out our dataset. This means we are throwing away many data points. We use `seed=1` so everyone gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "positive_reviews_raw = reviews[reviews['sentiment'] == 1]\n",
    "negative_reviews_raw  = reviews[reviews['sentiment'] == -1]\n",
    "\n",
    "# Undersample the reviews.\n",
    "percentage = len(negative_reviews_raw)/float(len(positive_reviews_raw))\n",
    "negative_reviews = negative_reviews_raw\n",
    "positive_reviews = positive_reviews_raw.sample(percentage, seed=1)\n",
    "products = negative_reviews_raw.append(positive_reviews)\n",
    "\n",
    "print \"Percentage of positive reviews             :\", len(positive_reviews) / float(len(products))\n",
    "print \"Percentage of negative reviews             :\", len(negative_reviews) / float(len(products))\n",
    "print \"Total number of reviews in our new dataset :\", len(products)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 2/6\n",
    "Let us quickly explore more of this dataset.\n",
    "3. We count the number of positive and negative reviews.\n",
    "\n",
    "**TODO**: Modify the subset to contain similar numbers of positive and negative reviews, as the original dataset consisted primarily of positive reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '# of positive reviews =', len(reviews[reviews['sentiment']==1])\n",
    "print '# of negative reviews =', len(reviews[reviews['sentiment']==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print '# of positive balanced reviews =', len(products[products['sentiment']==1])\n",
    "print '# of negative balanced reviews =', len(products[products['sentiment']==-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reviews Extraction Phase: data preparation. \n",
    "## (desarrollado en el notebook Proyecto_DSC_DIRJAA)\n",
    "**Note:** \n",
    " - column review_clean with text cleaning developed in Proyecto_DSC_DIRJAA notebook\n",
    " - building features: la lista de palabras important_words.json se confecciona en el nb Proyecto_DSC_DIRJAA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are several ways of doing this. We use the built-in *count* function for Python lists. Each **review without punctuation, stopwords, etc** string is first split into individual words and the number of occurances of a given word is counted.\n",
    "1. Transform the reviews into word-counts (only for **important_words**, without punctuation, stopwords, etc)\n",
    "2. For each word in **important_words**, we compute a count for the number of times the word occurs in the review. We will store this count in a separate column (one for each word). The result of this feature processing is a single column for each word in **important_words** which keeps a count of the number of times the respective word occurs in the review text.\n",
    "\n",
    "Now, we will load these words from this JSON file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "with open('important_words.json', 'r') as f: # Reads the list of words\n",
    "    important_words = json.load(f)\n",
    "important_words = [str(s) for s in important_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print important_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    products[word] = products['review_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 3/6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The SFrame **products** now contains one column for each of the **important_words**. As an example, the column **perfect** contains a count of the number of times the word **perfect** occurs in each of the reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products['perfect']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, write some code to compute the number of product reviews that contain the word **perfect**.\n",
    "* First create a column called `contains_perfect` which is set to 1 if the count of the word **perfect** (stored in column **perfect**) is >= 1.\n",
    "* Sum the number of 1s in the column `contains_perfect`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products['contains_perfect'] = products['perfect'].apply(lambda s : +1 if s >= 1 else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "products['contains_perfect'].sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing logistic regression from scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## link function (estimating conditional probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from lecture that the link function is given by:\n",
    "$$\n",
    "P(y_i = +1 | \\mathbf{x}_i,\\mathbf{w}) = \\frac{1}{1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))},\n",
    "$$\n",
    "\n",
    "where the feature vector $h(\\mathbf{x}_i)$ represents the word counts of **important_words** in the review  $\\mathbf{x}_i$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "produces probablistic estimate for P(y_i = +1 | x_i, w).\n",
    "estimate ranges between 0 and 1.\n",
    "'''\n",
    "\n",
    "def predict_probability(feature_matrix, coefficients):\n",
    "    # Take dot product of feature_matrix and coefficients  \n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    \n",
    "    # Compute P(y_i = +1 | x_i, w) using the link function\n",
    "    predictions = 1.0 / (1.0 + np.exp(-scores))\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How the link function works with matrix algebra?\n",
    "\n",
    "Since the word counts are stored as columns in **feature_matrix**, each $i$-th row of the matrix corresponds to the feature vector $h(\\mathbf{x}_i)$:\n",
    "$$\n",
    "[\\text{feature_matrix}] =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T \\\\\n",
    "h(\\mathbf{x}_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\n",
    "\\end{array}\n",
    "\\right] =\n",
    "\\left[\n",
    "\\begin{array}{cccc}\n",
    "h_0(\\mathbf{x}_1) & h_1(\\mathbf{x}_1) & \\cdots & h_D(\\mathbf{x}_1) \\\\\n",
    "h_0(\\mathbf{x}_2) & h_1(\\mathbf{x}_2) & \\cdots & h_D(\\mathbf{x}_2) \\\\\n",
    "\\vdots & \\vdots & \\ddots & \\vdots \\\\\n",
    "h_0(\\mathbf{x}_N) & h_1(\\mathbf{x}_N) & \\cdots & h_D(\\mathbf{x}_N)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$\n",
    "\n",
    "By the rules of matrix multiplication, the score vector containing elements $\\mathbf{w}^T h(\\mathbf{x}_i)$ is obtained by multiplying **feature_matrix** and the coefficient vector $\\mathbf{w}$.\n",
    "$$\n",
    "[\\text{score}] =\n",
    "[\\text{feature_matrix}]\\mathbf{w} =\n",
    "\\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T \\\\\n",
    "h(\\mathbf{x}_2)^T \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\n",
    "\\end{array}\n",
    "\\right]\n",
    "\\mathbf{w}\n",
    "= \\left[\n",
    "\\begin{array}{c}\n",
    "h(\\mathbf{x}_1)^T\\mathbf{w} \\\\\n",
    "h(\\mathbf{x}_2)^T\\mathbf{w} \\\\\n",
    "\\vdots \\\\\n",
    "h(\\mathbf{x}_N)^T\\mathbf{w}\n",
    "\\end{array}\n",
    "\\right]\n",
    "= \\left[\n",
    "\\begin{array}{c}\n",
    "\\mathbf{w}^T h(\\mathbf{x}_1) \\\\\n",
    "\\mathbf{w}^T h(\\mathbf{x}_2) \\\\\n",
    "\\vdots \\\\\n",
    "\\mathbf{w}^T h(\\mathbf{x}_N)\n",
    "\\end{array}\n",
    "\\right]\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute derivative of log likelihood with respect to a single coefficient\n",
    "\n",
    "Recall:\n",
    "$$\n",
    "\\frac{\\partial\\ell}{\\partial w_j} = \\sum_{i=1}^N h_j(\\mathbf{x}_i)\\left(\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})\\right)\n",
    "$$\n",
    "\n",
    "Function that computes the derivative of log likelihood with respect to a single coefficient $w_j$. The function accepts two arguments:\n",
    "* `errors` vector containing $\\mathbf{1}[y_i = +1] - P(y_i = +1 | \\mathbf{x}_i, \\mathbf{w})$ for all $i$.\n",
    "* `feature` vector containing $h_j(\\mathbf{x}_i)$  for all $i$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def feature_derivative(errors, feature):     \n",
    "    # Compute the dot product of errors and feature\n",
    "    derivative = np.dot(errors, feature)\n",
    "    \n",
    "    # Return the derivative\n",
    "    return derivative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We introduced a transformation of this likelihood---called the log likelihood---that simplifies the derivation of the gradient and is more numerically stable.  Due to its numerical stability, we will use the log likelihood instead of the likelihood to assess the algorithm.\n",
    "\n",
    "The log likelihood is computed using the following formula (see the advanced optional video if you are curious about the derivation of this equation):\n",
    "\n",
    "$$\\ell\\ell(\\mathbf{w}) = \\sum_{i=1}^N \\Big( (\\mathbf{1}[y_i = +1] - 1)\\mathbf{w}^T h(\\mathbf{x}_i) - \\ln\\left(1 + \\exp(-\\mathbf{w}^T h(\\mathbf{x}_i))\\right) \\Big) $$\n",
    "\n",
    "Function to compute the log likelihood for the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def compute_log_likelihood(feature_matrix, sentiment, coefficients):\n",
    "    indicator = (sentiment==+1)\n",
    "    scores = np.dot(feature_matrix, coefficients)\n",
    "    logexp = np.log(1. + np.exp(-scores))\n",
    "    \n",
    "    # Simple check to prevent overflow\n",
    "    mask = np.isinf(logexp)\n",
    "    logexp[mask] = -scores[mask]\n",
    "    \n",
    "    lp = np.sum((indicator-1)*scores - logexp)\n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taking gradient steps\n",
    "Now we are ready to implement our own logistic regression. \n",
    "\n",
    "Function to solve the logistic regression model using gradient ascent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def logistic_regression(feature_matrix, sentiment, initial_coefficients, step_size, max_iter):\n",
    "    coefficients = np.array(initial_coefficients) # make sure it's a numpy array\n",
    "    for itr in xrange(max_iter):\n",
    "\n",
    "        # Predict P(y_i = +1|x_i,w) using your predict_probability() function\n",
    "        predictions = predict_probability(feature_matrix, coefficients)\n",
    "        \n",
    "        # Compute indicator value for (y_i = +1)\n",
    "        indicator = (sentiment==+1)\n",
    "        \n",
    "        # Compute the errors as indicator - predictions\n",
    "        errors = indicator - predictions\n",
    "        for j in xrange(len(coefficients)): # loop over each coefficient\n",
    "            \n",
    "            # Recall that feature_matrix[:,j] is the feature column associated with coefficients[j].\n",
    "            # Compute the derivative for coefficients[j]. Save it in a variable called derivative\n",
    "            derivative = np.dot(errors,feature_matrix[:,j])\n",
    "            \n",
    "            # add the step size times the derivative to the current coefficient\n",
    "            coefficients[j] = coefficients[j] + derivative*step_size\n",
    "        \n",
    "        # Checking whether log likelihood is increasing\n",
    "        if itr <= 15 or (itr <= 100 and itr % 10 == 0) or (itr <= 1000 and itr % 100 == 0) \\\n",
    "        or (itr <= 10000 and itr % 1000 == 0) or itr % 10000 == 0:\n",
    "            lp = compute_log_likelihood(feature_matrix, sentiment, coefficients)\n",
    "            print 'iteration %*d: log likelihood of observed labels = %.8f' % \\\n",
    "                (int(np.ceil(np.log10(max_iter))), itr, lp)\n",
    "    return coefficients"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resolving a sentiment classifier with logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data into training and test sets\n",
    "Let's perform a train/test split with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = products.random_split(.8, seed=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "print '# of total reviews =', len(products)\n",
    "print '# of positive reviews on all data =', len(products[products['sentiment']==1])\n",
    "print '# of negative reviews on all data =', len(products[products['sentiment']==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "print '# of train_data reviews =', len(train_data)\n",
    "print '# of positive reviews on train data =', len(train_data[train_data['sentiment']==1])\n",
    "print '# of negative reviews on train data =', len(train_data[train_data['sentiment']==-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "print '# of test_data reviews =', len(test_data)\n",
    "print '# of positive reviews on test data =', len(test_data[test_data['sentiment']==1])\n",
    "print '# of negative reviews on test data =', len(test_data[test_data['sentiment']==-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SFrame to NumPy array\n",
    "NumPy is a powerful library for doing matrix manipulation. Let us convert our data to matrices and then implement our algorithms with matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Function that extracts columns from an SFrame and converts them into a NumPy array. Two arrays are returned: one representing features and another representing class labels. The feature matrix includes an additional column 'intercept' to take account of the intercept term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "def get_numpy_data(data_sframe, features, label):\n",
    "    data_sframe['intercept'] = 1\n",
    "    features = ['intercept'] + features\n",
    "    features_sframe = data_sframe[features]\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    label_sarray = data_sframe[label]\n",
    "    label_array = label_sarray.to_numpy()\n",
    "    return(feature_matrix, label_array)\n",
    "\n",
    "def get_numpy_feature_matrix(data_sframe, features):\n",
    "    data_sframe['intercept'] = 1\n",
    "    features = ['intercept'] + features\n",
    "    features_sframe = data_sframe[features]\n",
    "    feature_matrix = features_sframe.to_numpy()\n",
    "    return(feature_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us convert the train_data into NumPy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "train_feature_matrix, train_sentiment = get_numpy_data(train_data, important_words, 'sentiment') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_feature_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating the sentiment classifier on the training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Warning: This may take a few minutes...\n",
    "#Atención el número de ceros debe coincidir con el número de palabras más uno.\n",
    "sentiment_model_coefficients = logistic_regression(train_feature_matrix, train_sentiment, initial_coefficients=np.zeros(train_feature_matrix.shape[1]),\n",
    "                                   step_size=1e-7, max_iter=301)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class predictions from scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Class predictions for a data point $\\mathbf{x}$ can be computed from the coefficients $\\mathbf{w}$ using the following formula:\n",
    "$$\n",
    "\\hat{y}_i = \n",
    "\\left\\{\n",
    "\\begin{array}{ll}\n",
    "      +1 & \\mathbf{x}_i^T\\mathbf{w} > 0 \\\\\n",
    "      -1 & \\mathbf{x}_i^T\\mathbf{w} \\leq 0 \\\\\n",
    "\\end{array} \n",
    "\\right.\n",
    "$$\n",
    "\n",
    "Now, we will write some code to compute class predictions. We will do this in two steps:\n",
    "* **Step 1**: First compute the **scores** using **feature_matrix** and **coefficients** using a dot product.\n",
    "* **Step 2**: Using the formula above, compute the class predictions from the scores.\n",
    "\n",
    "Step 1 can be implemented as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Compute the scores as a dot product between feature_matrix and coefficients.\n",
    "scores = np.dot(train_feature_matrix, sentiment_model_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 2: compute the class predictions using the **scores** obtained above:\n",
    "train_sentiment_predictions = map((lambda score: +1 if score > 0 else -1), scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Measuring accuracy of the model\n",
    "\n",
    "We will now measure the classification accuracy of the model. \n",
    "In accuracy, instead of measuring the number of errors, we measure the number of correct classifications.\n",
    "So the ratio here is number of correct divided by total number of sentences. \n",
    "In terms of accuracy, the best possible value is 1, I've got all the sentences right. \n",
    "The classification accuracy can be computed as follows:\n",
    "\n",
    "$$\n",
    "\\mbox{accuracy} = \\frac{\\mbox{# correctly classified data points}}{\\mbox{# total data points}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "num_mistakes = (train_sentiment != train_sentiment_predictions).sum()\n",
    "accuracy = 1.0 * (len(train_data) - num_mistakes) / len(train_data)\n",
    "print \"-----------------------------------------------------\"\n",
    "print '# Reviews   correctly classified =', len(train_data) - num_mistakes\n",
    "print '# Reviews incorrectly classified =', num_mistakes\n",
    "print '# Reviews total                  =', len(train_data)\n",
    "print \"-----------------------------------------------------\"\n",
    "print 'Accuracy = %.2f' % accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 4/6\n",
    "## Which words contribute most to positive & negative sentiments?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We were able to compute the \"**most positive words**\". These are words that correspond most strongly with positive reviews. In order to do this, we will first do the following:\n",
    "* Treat each coefficient as a tuple, i.e. (**word**, **coefficient_value**).\n",
    "* Sort all the (**word**, **coefficient_value**) tuples by **coefficient_value** in descending order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sentiment_model_coefficients_without_intercept = list(sentiment_model_coefficients[1:]) # exclude intercept\n",
    "word_coefficient_tuples = [(word, coefficient) for word, coefficient in zip(important_words, sentiment_model_coefficients_without_intercept)]\n",
    "word_coefficient_tuples = sorted(word_coefficient_tuples, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, **word_coefficient_tuples** contains a sorted list of (**word**, **coefficient_value**) tuples. The first 10 elements in this list correspond to the words that are most positive."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ten \"most positive\" words\n",
    "\n",
    "Now, we compute the 10 words that have the most positive coefficient values. These words are associated with positive sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_coefficient_tuples[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ten \"most negative\" words\n",
    "\n",
    "Next, we repeat this exercise on the 10 most negative words.  That is, we compute the 10 words that have the most negative coefficient values. These words are associated with negative sentiment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "word_coefficient_tuples[len(word_coefficient_tuples)-10:len(word_coefficient_tuples)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Set. Making predictions with logistic regression\n",
    "Now that a model is trained, we can make predictions on the **test data**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to convert test_data into the sparse matrix format first.\n",
    "# Warning: This may take a few minutes...\n",
    "test_feature_matrix, test_sentiment = get_numpy_data(test_data, important_words, 'sentiment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 1: Compute the scores as a dot product between feature_matrix and coefficients.\n",
    "scores = np.dot(test_feature_matrix, sentiment_model_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Step 2: compute the class predictions using the **scores** obtained above:\n",
    "test_sentiment_predictions = map((lambda score: +1 if score > 0 else -1), scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Accuracy on Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_test_mistakes = (test_sentiment != test_sentiment_predictions).sum()\n",
    "accuracy = 1.0 * (len(test_data) - num_test_mistakes) / len(test_data)\n",
    "print \"-----------------------------------------------------\"\n",
    "print '# Reviews   correctly classified =', len(test_data) - num_test_mistakes\n",
    "print '# Reviews incorrectly classified =', num_test_mistakes\n",
    "print '# Reviews total                  =', len(test_data)\n",
    "print \"-----------------------------------------------------\"\n",
    "print 'Accuracy = %.2f' % accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 5/6\n",
    "## Applying the learned model to understand sentiment for reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data['predicted_sentiment'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most positive reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_data.sort('predicted_sentiment', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data[['predicted_sentiment','sentiment','overall','reviewText']][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most positive review\n",
    "test_data[0]['reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most negative reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = test_data.sort('predicted_sentiment', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data[['predicted_sentiment','sentiment','overall','reviewText']][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Most negative review\n",
    "test_data[0]['reviewText']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Data 6/6: \n",
    "# TODO: Applying the learned model to discover insights on twitter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tweets Extraction Phase: data preparation. \n",
    "### (desarrollado en el notebook Proyecto_DSC_DIRJAA)\n",
    "**Note:** \n",
    " - column text_clean with text cleaning developed in Proyecto_DSC_DIRJAA notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_data = sframe.SFrame('us_tweets.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for word in important_words:\n",
    "    tweets_data[word] = tweets_data['text_clean'].apply(lambda s : s.split().count(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# We need to convert test_data into the sparse matrix format first.\n",
    "# Warning: This may take a few minutes...\n",
    "tweets_feature_matrix  = get_numpy_feature_matrix(tweets_data, important_words) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Step 1: Compute the scores as a dot product between feature_matrix and coefficients.\n",
    "scores = np.dot(tweets_feature_matrix, sentiment_model_coefficients)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_data['predicted_sentiment'] = scores"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Export to json array to d3 visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Five most positive tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_data = tweets_data.sort('predicted_sentiment', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_data[['predicted_sentiment','text']][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_tweets_data = tweets_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "positive_tweets_data.export_json('d3/data/USA-positive.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Five most negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_data = tweets_data.sort('predicted_sentiment', ascending=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tweets_data[['predicted_sentiment','text']][0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_tweets_data = tweets_data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "negative_tweets_data.export_json('d3/data/USA-negative.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Most positives and negatives tweets of EEUU dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import IFrame\n",
    "\n",
    "def serve_html():\n",
    "    fn= './d3/html/d3_6_US_map_tooltips.html'\n",
    "    return IFrame(fn, 985, 570)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"985\"\n",
       "            height=\"570\"\n",
       "            src=\"./d3/html/d3_6_US_map_tooltips.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x106134510>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serve_html()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
